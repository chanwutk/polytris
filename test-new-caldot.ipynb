{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_h264(input_path: str):\n",
    "    # Convert video to H.264 codec using ffmpeg\n",
    "    h264_output_path = input_path.replace('.mp4', '.h264.mp4')\n",
    "\n",
    "    # Run FFMPEG command to convert to H.264\n",
    "    cmd = [\n",
    "        'ffmpeg', '-y',  # -y to overwrite output file\n",
    "        '-loglevel', 'quiet',  # silence FFMPEG output\n",
    "        '-i', input_path,  # input file\n",
    "        '-c:v', 'libx264',  # H.264 codec\n",
    "        '-preset', 'fast',  # encoding preset\n",
    "        '-crf', '28',  # constant rate factor (lower quality, smaller file)\n",
    "        '-profile:v', 'baseline',  # baseline profile for better compatibility\n",
    "        '-level', '3.0',  # H.264 level for broader device support\n",
    "        '-movflags', '+faststart',  # optimize for streaming/web playback\n",
    "        '-pix_fmt', 'yuv420p',  # pixel format for maximum compatibility\n",
    "        '-tune', 'fastdecode',  # optimize for faster decoding\n",
    "        h264_output_path\n",
    "    ]\n",
    "\n",
    "    print(f\"Converting {input_path} to H.264 format...\")\n",
    "    subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(f\"H.264 video saved to {h264_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf47a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model\n",
    "model = YOLO(\"./runs/detect/train13/weights/best.pt\")\n",
    "\n",
    "# Paths to video and annotation files\n",
    "video_path = '/otif-dataset/dataset/caldot1/train/video/0.mp4'\n",
    "annotation_path = '/otif-dataset/dataset/caldot1/train/yolov3-704x480/0.json'\n",
    "output_video_path = './0_annotated_with_predictions.mp4'\n",
    "\n",
    "# Load annotation file\n",
    "with open(annotation_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Print annotation structure to understand the format\n",
    "print(\"Annotation keys:\", annotations.keys() if isinstance(annotations, dict) else \"List with\", len(annotations), \"items\")\n",
    "if isinstance(annotations, dict):\n",
    "    print(\"Sample keys:\", list(annotations.keys())[:5])\n",
    "    if 'annotations' in annotations:\n",
    "        print(\"Sample annotation:\", annotations['annotations'][0] if annotations['annotations'] else \"Empty\")\n",
    "elif isinstance(annotations, list):\n",
    "    print(\"Sample item:\", annotations[0] if annotations else \"Empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), f\"Could not open video {video_path}\"\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties: {width}x{height}, {fps} FPS, {frame_count} frames\")\n",
    "\n",
    "# Create video writer\n",
    "fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')\n",
    "writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "assert writer.isOpened(), f\"Could not create video writer for {output_video_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ec281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse annotations into a dictionary keyed by frame index\n",
    "# Handle different annotation formats\n",
    "frame_annotations = {}\n",
    "\n",
    "if isinstance(annotations, dict):\n",
    "    # COCO format or similar\n",
    "    if 'annotations' in annotations and 'images' in annotations:\n",
    "        # COCO format: map image_id to frame_index, then annotations to frames\n",
    "        image_id_to_frame = {}\n",
    "        for img in annotations['images']:\n",
    "            # Try to extract frame index from filename or use image_id\n",
    "            if 'frame_index' in img:\n",
    "                image_id_to_frame[img['id']] = img['frame_index']\n",
    "            elif 'file_name' in img:\n",
    "                # Try to extract frame number from filename\n",
    "                filename = img['file_name']\n",
    "                try:\n",
    "                    frame_idx = int(filename.split('_')[-1].split('.')[0])\n",
    "                    image_id_to_frame[img['id']] = frame_idx\n",
    "                except:\n",
    "                    image_id_to_frame[img['id']] = img['id']\n",
    "            else:\n",
    "                image_id_to_frame[img['id']] = img['id']\n",
    "        \n",
    "        # Group annotations by frame\n",
    "        for ann in annotations['annotations']:\n",
    "            image_id = ann['image_id']\n",
    "            frame_idx = image_id_to_frame.get(image_id, image_id)\n",
    "            if frame_idx not in frame_annotations:\n",
    "                frame_annotations[frame_idx] = []\n",
    "            \n",
    "            # Extract bbox: COCO format is [x, y, width, height]\n",
    "            bbox = ann['bbox']\n",
    "            x, y, w, h = bbox\n",
    "            # Convert to [x1, y1, x2, y2] format\n",
    "            frame_annotations[frame_idx].append([x, y, x + w, y + h])\n",
    "    elif 'frames' in annotations:\n",
    "        # Custom format with frames key\n",
    "        for frame_data in annotations['frames']:\n",
    "            frame_idx = frame_data.get('frame_idx', frame_data.get('frame_index', 0))\n",
    "            frame_annotations[frame_idx] = frame_data.get('boxes', frame_data.get('detections', []))\n",
    "    else:\n",
    "        # Dictionary with frame indices as keys\n",
    "        frame_annotations = annotations\n",
    "elif isinstance(annotations, list):\n",
    "    # List format: each item is a frame annotation\n",
    "    for idx, frame_ann in enumerate(annotations):\n",
    "        if isinstance(frame_ann, dict):\n",
    "            frame_idx = frame_ann.get('frame_idx', frame_ann.get('frame_index', idx))\n",
    "            frame_annotations[frame_idx] = frame_ann.get('boxes', frame_ann.get('detections', frame_ann.get('annotations', [])))\n",
    "        elif isinstance(frame_ann, list):\n",
    "            # Direct list of boxes\n",
    "            frame_annotations[idx] = frame_ann\n",
    "\n",
    "print(f\"Loaded annotations for {len(frame_annotations)} frames\")\n",
    "print(f\"Frame indices range: {min(frame_annotations.keys()) if frame_annotations else 'N/A'} to {max(frame_annotations.keys()) if frame_annotations else 'N/A'}\")\n",
    "if frame_annotations:\n",
    "    sample_frame = list(frame_annotations.keys())[0]\n",
    "    print(f\"Sample frame {sample_frame} has {len(frame_annotations[sample_frame])} annotations\")\n",
    "    if frame_annotations[sample_frame]:\n",
    "        print(f\"Sample annotation format: {frame_annotations[sample_frame][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to draw bounding boxes\n",
    "def draw_box(frame, box, color, thickness=2):\n",
    "    \"\"\"Draw a bounding box on the frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: OpenCV frame image\n",
    "        box: Bounding box in various formats (dict, list, tuple)\n",
    "        color: BGR color tuple (e.g., (0, 255, 0) for green)\n",
    "        thickness: Line thickness\n",
    "    \"\"\"\n",
    "    # Handle different box formats\n",
    "    if isinstance(box, dict):\n",
    "        # Dictionary format with keys like 'left', 'top', 'right', 'bottom'\n",
    "        if 'left' in box and 'top' in box and 'right' in box and 'bottom' in box:\n",
    "            x1, y1, x2, y2 = box['left'], box['top'], box['right'], box['bottom']\n",
    "        elif 'x1' in box and 'y1' in box and 'x2' in box and 'y2' in box:\n",
    "            x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']\n",
    "        elif 'x' in box and 'y' in box and 'width' in box and 'height' in box:\n",
    "            # COCO format: [x, y, width, height]\n",
    "            x1 = box['x']\n",
    "            y1 = box['y']\n",
    "            x2 = x1 + box['width']\n",
    "            y2 = y1 + box['height']\n",
    "        else:\n",
    "            return\n",
    "    elif isinstance(box, (list, tuple)) and len(box) >= 4:\n",
    "        if len(box) == 4:\n",
    "            # [x1, y1, x2, y2] or [x, y, w, h]\n",
    "            x1, y1, x2_or_w, y2_or_h = box\n",
    "            # Check if it's [x, y, w, h] or [x1, y1, x2, y2]\n",
    "            if x2_or_w < x1 or y2_or_h < y1:\n",
    "                # Likely [x, y, w, h] format\n",
    "                x1, y1, w, h = box\n",
    "                x2, y2 = x1 + w, y1 + h\n",
    "            else:\n",
    "                # [x1, y1, x2, y2] format\n",
    "                x1, y1, x2, y2 = box\n",
    "        else:\n",
    "            # [track_id, x1, y1, x2, y2] or similar\n",
    "            x1, y1, x2, y2 = box[-4:]\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Convert to integers and ensure they're within frame bounds\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    x1 = max(0, min(x1, frame.shape[1]))\n",
    "    y1 = max(0, min(y1, frame.shape[0]))\n",
    "    x2 = max(0, min(x2, frame.shape[1]))\n",
    "    y2 = max(0, min(y2, frame.shape[0]))\n",
    "    \n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b18f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen video for processing (since we already read properties)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Process each frame and draw bounding boxes\n",
    "# Green for ground truth annotations, Red for model predictions\n",
    "GROUND_TRUTH_COLOR = (0, 255, 0)  # Green in BGR\n",
    "PREDICTION_COLOR = (0, 0, 255)    # Red in BGR\n",
    "\n",
    "frame_idx = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Get ground truth annotations for this frame\n",
    "    gt_boxes = frame_annotations.get(frame_idx, [])\n",
    "    \n",
    "    # Draw ground truth bounding boxes in green\n",
    "    for box in gt_boxes:\n",
    "        draw_box(frame, box, GROUND_TRUTH_COLOR, thickness=2)\n",
    "    \n",
    "    # Run YOLO model on this frame to get predictions\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    # Draw model predictions in red\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Get bounding box coordinates (xyxy format)\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            # Get confidence score\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            # Draw prediction box\n",
    "            draw_box(frame, [x1, y1, x2, y2], PREDICTION_COLOR, thickness=2)\n",
    "            # Optionally draw confidence score\n",
    "            label = f\"{conf:.2f}\"\n",
    "            cv2.putText(frame, label, (int(x1), int(y1) - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, PREDICTION_COLOR, 2)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    writer.write(frame)\n",
    "    \n",
    "    frame_idx += 1\n",
    "    \n",
    "    if frame_idx % 100 == 0:\n",
    "        print(f\"Processed {frame_idx}/{frame_count} frames\")\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "print(f\"Video saved to {output_video_path}\")\n",
    "to_h264(output_video_path)\n",
    "print(f\"H264 Video saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv3 (polyis) and Faster R-CNN models\n",
    "from polyis.models import detector as polyis_detector\n",
    "from polyis.models.detector import detect as polyis_detect\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load YOLOv3 detector via polyis detector utility (dataset_name='caldot1')\n",
    "print(\"Loading YOLOv3 detector (dataset 'caldot1')...\")\n",
    "yolov3_detector = polyis_detector.get_detector(dataset_name='caldot1', gpu_id=0, batch_size=1)\n",
    "print(\"YOLOv3 detector loaded.\")\n",
    "\n",
    "# Load Faster R-CNN trained weights without downloading backbone (no internet)\n",
    "fasterrcnn_weights_path = '/polyis-data/fasterrcnn_output/fasterrcnn_epoch_50.pth'\n",
    "print(f\"Loading Faster R-CNN weights from: {fasterrcnn_weights_path}\")\n",
    "\n",
    "# Build model architecture (2 classes: 1 object class + background)\n",
    "faster_rcnn_model = fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None)\n",
    "in_features = faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features  # type: ignore\n",
    "faster_rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "checkpoint = torch.load(fasterrcnn_weights_path, map_location=DEVICE)\n",
    "state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "faster_rcnn_model.load_state_dict(state_dict)\n",
    "faster_rcnn_model.to(DEVICE)\n",
    "faster_rcnn_model.eval()\n",
    "print(\"Faster R-CNN model loaded and set to eval mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three separate annotated videos: one for each detector with ground truth\n",
    "# Video 1: Ground truth + YOLOv11\n",
    "# Video 2: Ground truth + YOLOv3\n",
    "# Video 3: Ground truth + Faster R-CNN\n",
    "\n",
    "output_yolov11_path = './0_annotated_yolov11.mp4'\n",
    "output_yolov3_path = './0_annotated_yolov3.mp4'\n",
    "output_frcnn_path = './0_annotated_frcnn.mp4'\n",
    "\n",
    "# Get video properties\n",
    "cap_props = cv2.VideoCapture(video_path)\n",
    "assert cap_props.isOpened(), f\"Could not open video {video_path}\"\n",
    "fps_all = cap_props.get(cv2.CAP_PROP_FPS)\n",
    "width_all = int(cap_props.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height_all = int(cap_props.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count_all = int(cap_props.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap_props.release()\n",
    "\n",
    "print(f\"Video properties: {width_all}x{height_all}, {fps_all} FPS, {frame_count_all} frames\")\n",
    "\n",
    "# Colors for different sources\n",
    "COLOR_GT = (0, 255, 0)        # Green\n",
    "COLOR_YOLOV11 = (0, 0, 255)   # Red\n",
    "COLOR_YOLOV3 = (255, 0, 0)    # Blue\n",
    "COLOR_FRCNN = (0, 255, 255)   # Yellow\n",
    "\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# Create video writers\n",
    "fourcc_all = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')\n",
    "writer_yolov11 = cv2.VideoWriter(output_yolov11_path, fourcc_all, fps_all, (width_all, height_all))\n",
    "writer_yolov3 = cv2.VideoWriter(output_yolov3_path, fourcc_all, fps_all, (width_all, height_all))\n",
    "writer_frcnn = cv2.VideoWriter(output_frcnn_path, fourcc_all, fps_all, (width_all, height_all))\n",
    "\n",
    "assert writer_yolov11.isOpened(), f\"Could not create video writer for {output_yolov11_path}\"\n",
    "assert writer_yolov3.isOpened(), f\"Could not create video writer for {output_yolov3_path}\"\n",
    "assert writer_frcnn.isOpened(), f\"Could not create video writer for {output_frcnn_path}\"\n",
    "\n",
    "print(f\"Creating three separate annotated videos...\")\n",
    "print(f\"  1. YOLOv11: {output_yolov11_path}\")\n",
    "print(f\"  2. YOLOv3: {output_yolov3_path}\")\n",
    "print(f\"  3. Faster R-CNN: {output_frcnn_path}\")\n",
    "\n",
    "# Open video for processing\n",
    "cap_all = cv2.VideoCapture(video_path)\n",
    "frame_idx_all = 0\n",
    "\n",
    "while cap_all.isOpened():\n",
    "    ret, frame = cap_all.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Create separate copies for each video\n",
    "    frame_yolov11 = frame.copy()\n",
    "    frame_yolov3 = frame.copy()\n",
    "    frame_frcnn = frame.copy()\n",
    "\n",
    "    # Ground truth boxes (same for all)\n",
    "    gt_boxes = frame_annotations.get(frame_idx_all, [])\n",
    "    \n",
    "    # Draw GT on all frames\n",
    "    for box in gt_boxes:\n",
    "        draw_box(frame_yolov11, box, COLOR_GT, thickness=2)\n",
    "        draw_box(frame_yolov3, box, COLOR_GT, thickness=2)\n",
    "        draw_box(frame_frcnn, box, COLOR_GT, thickness=2)\n",
    "\n",
    "    # YOLOv11 predictions\n",
    "    results_v11 = model(frame, verbose=False)\n",
    "    for result in results_v11:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0].cpu().numpy())\n",
    "            if conf < CONF_THRESHOLD:\n",
    "                continue\n",
    "            draw_box(frame_yolov11, [x1, y1, x2, y2], COLOR_YOLOV11, thickness=2)\n",
    "            cv2.putText(frame_yolov11, f\"v11:{conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR_YOLOV11, 1)\n",
    "\n",
    "    # YOLOv3 predictions\n",
    "    yolo3_preds = polyis_detect(frame, yolov3_detector, threshold=CONF_THRESHOLD)\n",
    "    if yolo3_preds is not None:\n",
    "        for pred in yolo3_preds:\n",
    "            x1, y1, x2, y2, conf = pred\n",
    "            if conf < CONF_THRESHOLD:\n",
    "                continue\n",
    "            draw_box(frame_yolov3, [x1, y1, x2, y2], COLOR_YOLOV3, thickness=2)\n",
    "            cv2.putText(frame_yolov3, f\"v3:{conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR_YOLOV3, 1)\n",
    "\n",
    "    # Faster R-CNN predictions\n",
    "    with torch.no_grad():\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        tensor = torch.from_numpy(rgb).permute(2, 0, 1).float().to(DEVICE) / 255.0\n",
    "        outputs = faster_rcnn_model([tensor])[0]\n",
    "        frcnn_boxes = outputs['boxes'].detach().cpu().numpy()\n",
    "        frcnn_scores = outputs['scores'].detach().cpu().numpy()\n",
    "        for (x1, y1, x2, y2), conf in zip(frcnn_boxes, frcnn_scores):\n",
    "            if float(conf) < CONF_THRESHOLD:\n",
    "                continue\n",
    "            draw_box(frame_frcnn, [x1, y1, x2, y2], COLOR_FRCNN, thickness=2)\n",
    "            cv2.putText(frame_frcnn, f\"fr:{conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR_FRCNN, 1)\n",
    "\n",
    "    # Write frames to respective videos\n",
    "    writer_yolov11.write(frame_yolov11)\n",
    "    writer_yolov3.write(frame_yolov3)\n",
    "    writer_frcnn.write(frame_frcnn)\n",
    "    \n",
    "    frame_idx_all += 1\n",
    "    if frame_idx_all % 100 == 0:\n",
    "        print(f\"Processed {frame_idx_all}/{frame_count_all} frames\")\n",
    "\n",
    "cap_all.release()\n",
    "writer_yolov11.release()\n",
    "writer_yolov3.release()\n",
    "writer_frcnn.release()\n",
    "\n",
    "print(f\"\\nAll videos saved successfully!\")\n",
    "print(f\"Converting to H.264 format...\")\n",
    "to_h264(output_yolov11_path)\n",
    "to_h264(output_yolov3_path)\n",
    "to_h264(output_frcnn_path)\n",
    "print(f\"H.264 videos saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
