import json
import os
import subprocess
import time
import typing
import multiprocessing as mp
import functools
import queue
import random

import cv2
import numpy as np
from rich import progress
import torch

if typing.TYPE_CHECKING:
    import altair as alt
    import pandas as pd

SOURCE_DIR = '/polyis-data/sources'
DATASETS_DIR = '/polyis-data/datasets'
DATA_RAW_DIR = '/polyis-data/video-datasets-raw'
DATA_DIR = '/polyis-data/video-datasets'
CACHE_DIR = '/polyis-cache'
TILE_SIZES = [60]

GS_DATASETS_DIR = 'gs://polytris/polyis-data/datasets'
GS_CACHE = 'gs://polytris/polyis-cache'

GC_DATASETS_DIR = '/data/chanwutk/data/polyis-data/datasets'
GC_CACHE = '/data/chanwutk/data/polyis-cache'

# Define 10 distinct colors for track visualization (BGR format for OpenCV)
TRACK_COLORS = [
    (255, 0, 0),    # Blue
    (0, 255, 0),    # Green
    (0, 0, 255),    # Red
    (255, 255, 0),  # Cyan
    (255, 0, 255),  # Magenta
    (0, 255, 255),  # Yellow
    (128, 0, 255),  # Purple
    (255, 128, 0),  # Orange
    (0, 128, 255),  # Light Blue
    (255, 0, 128),  # Pink
]


video_frame_counts: dict[tuple[str, str], int] = {}


def get_video_frame_count(dataset: str, video: str) -> int:
    """
    Get the total number of frames in a video using OpenCV.

    Args:
        dataset (str): Dataset name
        video_name (str): Video name (with extension, e.g., 'te01.mp4')

    Returns:
        int: Total number of frames in the video
    """
    if (dataset, video) in video_frame_counts:
        return video_frame_counts[(dataset, video)]
    
    video_path = os.path.join(DATASETS_DIR, dataset, 'test', video)
    assert os.path.exists(video_path), f"Video file not found for {dataset}/{video}"
    
    # Open video and get frame count
    cap = cv2.VideoCapture(video_path)
    assert cap.isOpened(), f"Could not open video {video_path}"
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()

    video_frame_counts[(dataset, video)] = frame_count
    return frame_count


def get_num_frames(video_file_path: str) -> int:
    """
    Get the number of frames from a video file.
    
    Args:
        video_file_path (str): Path to the video file
        
    Returns:
        int: Number of frames in the video
        
    Raises:
        AssertionError: If the video file cannot be opened
    """
    cap = cv2.VideoCapture(video_file_path)
    assert cap.isOpened(), f"Could not open video {video_file_path}"
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    
    return frame_count


def format_time(**kwargs: float | int) -> list[dict[str, float | int | str]]:
    """
    Format timing information into a list of dictionaries.
    
    Args:
        **kwargs: Keyword arguments where keys are operation names and values are timing values
        
    Returns:
        list: List of dictionaries with 'op' (operation) and 'time' keys for each input argument
        
    Example:
        >>> format_time(read=1.5, detect=2.3)
        [{'op': 'read', 'time': 1.5}, {'op': 'detect', 'time': 2.3}]
    """
    return [{'op': op, 'time': time} for op, time in kwargs.items()]


def load_detection_results(cache_dir: str, dataset: str, video_file: str, tracking: bool = False, verbose: bool = False) -> list[dict]:
    """
    Load detection results from the JSONL file generated by 001_preprocess_groundtruth_detection.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tracking (bool): Whether to load tracking results instead of detection results
        verbose (bool): Whether to print verbose output
    Returns:
        list[dict]: list of frame detection results
        
    Raises:
        FileNotFoundError: If no detection results file is found
    """
    file = 'tracking.jsonl' if tracking else 'detections.jsonl'
    detection_path = os.path.join(cache_dir, dataset, 'execution', video_file, '000_groundtruth', file)
    
    if not os.path.exists(detection_path):
        raise FileNotFoundError(f"Detection results not found: {detection_path}")
    
    if verbose:
        print(f"Loading detection results from: {detection_path}")
    
    results = []
    with open(detection_path, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    if verbose:
        print(f"Loaded {len(results)} frame detections")
    return results


def load_tracking_results(cache_dir: str, dataset: str, video_file: str, verbose: bool = False) -> dict[int, list[list[float]]]:
    """
    Load tracking results from the JSONL file generated by 002_preprocess_groundtruth_tracking.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        verbose (bool): Whether to print verbose output
    Returns:
        dict[int, list[list[float]]]: dictionary mapping frame indices to lists of tracks
        
    Raises:
        FileNotFoundError: If no tracking results file is found
    """
    tracking_path = os.path.join(cache_dir, dataset, 'execution', video_file, '000_groundtruth', 'tracking.jsonl')
    
    if not os.path.exists(tracking_path):
        raise FileNotFoundError(f"Tracking results not found: {tracking_path}")
    
    if verbose:
        print(f"Loading tracking results from: {tracking_path}")
    
    frame_tracks = {}
    with open(tracking_path, 'r') as f:
        for line in f:
            if line.strip():
                frame_data = json.loads(line)
                frame_idx = frame_data['frame_idx']
                tracks = frame_data['tracks']
                frame_tracks[frame_idx] = tracks
    
    if verbose:
        print(f"Loaded tracking results for {len(frame_tracks)} frames")
    return frame_tracks


def interpolate_trajectory(trajectory: list[tuple[int, np.ndarray]], nxt: tuple[int, np.ndarray]) -> list[tuple[int, np.ndarray]]:
    """
    Perform linear interpolation between two trajectory points except the last point (nxt).
    
    Args:
        trajectory (list[tuple[int, np.ndarray]]): list of (frame_idx, detection) tuples
        nxt (tuple[int, np.ndarray]): Next detection point (frame_idx, detection)
        
    Returns:
        list[tuple[int, np.ndarray]]: list of interpolated points
    """
    extend: list[tuple[int, np.ndarray]] = []
    
    if len(trajectory) != 0:
        prv = trajectory[-1]
        assert prv[0] < nxt[0]
        prv_det = prv[1]
        nxt_det = nxt[1]
        dif_det = nxt_det - prv_det
        dif_det = dif_det.reshape(1, -1)

        scale = np.arange(0, nxt[0] - prv[0], dtype=np.float32).reshape(-1, 1) / (nxt[0] - prv[0])
        
        int_dets = (scale @ dif_det) + prv_det.reshape(1, -1)

        for idx, int_det in enumerate(int_dets[:-1]):
            extend.append((prv[0] + idx + 1, int_det))

    return extend


def register_tracked_detections(
    tracked_dets: np.ndarray,
    frame_idx: int,
    frame_tracks: dict[int, list[list[float]]],
    trajectories: dict[int, list[tuple[int, np.ndarray]]],
    no_interpolate: bool
):
    """
    Register tracked detections to frame tracks and trajectories.

    Args:
        tracked_dets (np.ndarray): Tracked detections
        frame_idx (int): Frame index
        frame_tracks (dict[int, list[list[float]]]): Frame tracks
        trajectories (dict[int, list[tuple[int, np.ndarray]]]): Trajectories
        no_interpolate (bool): Whether to not perform trajectory interpolation
    """

    if tracked_dets.size == 0:
        return

    if frame_idx not in frame_tracks:
        frame_tracks[frame_idx] = []

    for track in tracked_dets:
        # SORT returns: [x1, y1, x2, y2, track_id]
        x1, y1, x2, y2, track_id = track
        track_id = int(track_id)
        
        # # Convert to detection format: [track_id, x1, y1, x2, y2]
        # detection = [track_id, x1, y1, x2, y2]
        
        # # Add to frame tracks
        # if frame_idx not in frame_tracks:
        #     frame_tracks[frame_idx] = []
        # frame_tracks[frame_idx].append(detection)

        if track_id not in trajectories:
            trajectories[track_id] = []
        box_array = np.array([x1, y1, x2, y2], dtype=np.float16)
        
        # Add to trajectories for interpolation (if enabled)
        if no_interpolate:
            continue

        extend = interpolate_trajectory(trajectories[track_id],
                                        (frame_idx, box_array))
        
        # Add interpolated points to frame tracks
        for e in extend + [(frame_idx, box_array)]:
            e_frame_idx, e_box = e
            if e_frame_idx not in frame_tracks:
                frame_tracks[e_frame_idx] = []
            
            # Convert back to list format: [track_id, x1, y1, x2, y2]
            e_detection = [track_id, *e_box.tolist()]
            frame_tracks[e_frame_idx].append(e_detection)

            # Add interpolated points to trajectories
            trajectories[track_id].append((e_frame_idx, e_box))

        # trajectories[track_id].append((frame_idx, box_array))


def get_track_color(track_id: int, track_ids: list[int] | None = None) -> tuple[int, int, int]:
    """
    Get a color for a track ID by cycling through the predefined colors.
    If track_ids is specified, only those track IDs get colors, others get grey.
    
    Args:
        track_id (int): Track ID
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        
    Returns:
        tuple[int, int, int]: BGR color tuple
    """
    # If track_ids is specified and this track_id is not in the list, return grey
    if track_ids is not None and track_id not in track_ids:
        return (128, 128, 128)  # Grey color in BGR format
    
    # Otherwise, use the normal color cycling
    color_index = track_id % len(TRACK_COLORS)
    return TRACK_COLORS[color_index]


def overlapi(interval1: tuple[int, int], interval2: tuple[int, int]):
    """
    Check if two 1D intervals overlap.
    
    Args:
        interval1 (tuple[int, int]): First interval as (start, end)
        interval2 (tuple[int, int]): Second interval as (start, end)
        
    Returns:
        bool: True if the intervals overlap, False otherwise
    """
    return (
        (interval1[0] <= interval2[0] <= interval1[1]) or
        (interval1[0] <= interval2[1] <= interval1[1]) or
        (interval2[0] <= interval1[0] <= interval2[1]) or
        (interval2[0] <= interval1[1] <= interval2[1])
    )


def overlap(b1, b2):
    """
    Check if two 2D bounding boxes overlap.
    
    Args:
        b1: First bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        b2: Second bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        
    Returns:
        bool: True if the bounding boxes overlap in both x and y dimensions, False otherwise
    """
    return overlapi((b1[0], b1[2]), (b2[0], b2[2])) and overlapi((b1[1], b1[3]), (b2[1], b2[3]))


def get_precision(tp: int, fp: int) -> float:
    """
    Calculate precision.
    
    Args:
        tp (int): True positives
        fp (int): False positives
    """
    if (tp + fp) == 0:
        return 0.0
    return tp / (tp + fp)

def get_recall(tp: int, fn: int) -> float:
    """
    Calculate recall.
    
    Args:
        tp (int): True positives
        fn (int): False negatives
    """
    if (tp + fn) == 0:
        return 0.0
    return tp / (tp + fn)

def get_accuracy(tp: int, tn: int, fp: int, fn: int) -> float:

    """
    Calculate accuracy.
    
    Args:
        tp (int): True positives
        tn (int): True negatives
        fp (int): False positives
        fn (int): False negatives
    """
    if (tp + tn + fp + fn) == 0:
        return 0.0
    return (tp + tn) / (tp + tn + fp + fn)

def get_f1_score(tp: int, fp: int, fn: int) -> float:

    """
    Calculate F1 score.
    
    Args:
        tp (int): True positives
        fp (int): False positives
        fn (int): False negatives
    """
    if (get_precision(tp, fp) + get_recall(tp, fn)) == 0:
        return 0.0
    precision = get_precision(tp, fp)
    recall = get_recall(tp, fn)
    return 2. * (precision * recall) / (precision + recall)


def load_classification_results(cache_dir: str, dataset: str, video_file: str,
                                tilesize: int, classifier: str, verbose: bool = False,
                                execution_dir: bool = False) -> list:
    """
    Load classification results from the JSONL file generated by 020_exec_classify.py or 021_exec_classify_correct.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tilesize (int): Tile size used for classification
        classifier (str): Classifier name to use
        verbose (bool): Whether to print verbose output
        execution_dir (bool): Whether to look in execution directory (for 021_exec_classify_correct.py results)
        
    Returns:
        list: List of frame classification results, each containing frame data and classifications
        
    Raises:
        FileNotFoundError: If no classification results file is found
    """
    # Look for the classification results file
    if execution_dir:
        score_dir = os.path.join(cache_dir, dataset, 'execution', video_file, '020_relevancy', f'{classifier}_{tilesize}', 'score')
    else:
        score_dir = os.path.join(cache_dir, dataset, video_file, '020_relevancy', f'{classifier}_{tilesize}', 'score')
    
    # Use model scores
    expected_filename = 'score.jsonl'
    
    # Look for the specific results file
    results_file = os.path.join(score_dir, expected_filename)
    
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Classification results file not found: {results_file}")
    
    if verbose:
        print(f"Loading classification results from: {results_file}")
    
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    if verbose:
        print(f"Loaded {len(results)} frame classifications")
    return results


def create_tracker(tracker_name: str):
    """
    Create a tracker instance based on the specified algorithm.
    
    Args:
        tracker_name (str): Name of the tracking algorithm
        max_age (int): Maximum age for SORT tracker
        min_hits (int): Minimum hits for SORT tracker
        iou_threshold (float): IOU threshold for SORT tracker
        
    Returns:
        Tracker instance
        
    Raises:
        ValueError: If the tracker name is not supported
    """
    with open('configs/trackers.json', 'r') as f:
        if tracker_name == 'sort':
            # print(f"Creating SORT tracker with max_age={max_age}, min_hits={min_hits}, iou_threshold={iou_threshold}")
            from polyis.b3d.sort import Sort
            config = json.load(f)['sort']
            return Sort(max_age=config['max_age'], min_hits=config['min_hits'], iou_threshold=config['iou_threshold'])
        else:
            raise ValueError(f"Unknown tracker: {tracker_name}")


def create_visualization_frame(frame: np.ndarray, tracks: list[list[float]], frame_idx: int,
                               trajectory_history: dict[int, list[tuple[int, int, int]]], 
                               speed_up: int, track_ids: list[int] | None, detection_only: bool = False) -> np.ndarray | None:
    """
    Create a visualization frame by drawing bounding boxes and trajectories for all tracks.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        tracks (list[list[float]]): list of tracks for this frame
        frame_idx (int): Frame index for logging
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        speed_up (int): Speed up factor (process every Nth frame)
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, only show detections without trajectories, all boxes in green without track IDs
        
    Returns:
        np.ndarray | None: Frame with bounding boxes and trajectories drawn, or None if frame should be skipped
    """
    # First loop: Update trajectory history for all tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            track_id = int(track_id)
            
            # Calculate center of bounding box
            center_x = int((x1 + x2) // 2)
            center_y = int((y1 + y2) // 2)
            
            # Update trajectory history with frame timestamp
            if track_id not in trajectory_history:
                trajectory_history[track_id] = []
            trajectory_history[track_id].append((center_x, center_y, frame_idx))

    if frame_idx % speed_up != 0:
        return None
    
    # Create a copy of the frame for visualization
    vis_frame = frame.copy()
    
    if detection_only:
        # In detection-only mode, draw all bounding boxes in green without track IDs
        draw_track_bounding_boxes(vis_frame, tracks, track_ids, detection_only=True)
    else:
        # Draw bounding boxes and labels for tracks not in track_ids (grey)
        tracks_not_in_track_ids = [track for track in tracks if track[0] not in (track_ids or [])]
        draw_track_bounding_boxes(vis_frame, tracks_not_in_track_ids, track_ids)
        
        # Draw all trajectories with gradual fading
        draw_trajectories(vis_frame, trajectory_history, frame_idx, track_ids)
        
        # Draw bounding boxes and labels for tracks in track_ids (colored)
        tracks_in_track_ids = [track for track in tracks if track[0] in (track_ids or [])]
        draw_track_bounding_boxes(vis_frame, tracks_in_track_ids, track_ids)
    
    return vis_frame


def draw_track_bounding_boxes(vis_frame: np.ndarray, tracks: list[list[float]], 
                              track_ids: list[int] | None, detection_only: bool = False):
    """
    Draw bounding boxes and labels for all tracks on the visualization frame.
    
    Args:
        vis_frame (np.ndarray): Frame to draw on (modified in place)
        tracks (list[list[float]]): List of tracks for this frame
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, use green color for all boxes and hide track IDs
    """
    for track in tracks:
        assert len(track) >= 5, f"Track must have at least 5 elements: {track}"
        track_id, x1, y1, x2, y2 = track[:5]
        
        # Convert to integers for drawing
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        track_id = int(track_id)
        
        color = (0, 255, 0) if detection_only else get_track_color(track_id, track_ids)
        
        # Draw bounding box
        cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)
        
        # Draw track ID label only if not in detection-only mode
        if detection_only:
            continue

        label = str(track_id)
        font_scale = 0.6
        font_thickness = 2
        
        # Calculate text size and position
        (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                                                font_scale, font_thickness)
        
        # Position text above the bounding box
        text_x = x1
        text_y = max(y1 - 10, text_height + 5)
        
        # Draw text background for better visibility
        cv2.rectangle(vis_frame, (text_x - 2, text_y - text_height - 2), 
                        (text_x + text_width + 2, text_y + baseline + 2), 
                        color, -1)
        
        # Draw text
        cv2.putText(vis_frame, label, (text_x, text_y), 
                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)


def draw_trajectories(vis_frame: np.ndarray, trajectory_history: dict[int, list[tuple[int, int, int]]], 
                      frame_idx: int, track_ids: list[int] | None):
    """
    Draw all trajectories with gradual fading on the visualization frame.
    
    Args:
        vis_frame (np.ndarray): Frame to draw on (modified in place)
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        frame_idx (int): Current frame index for fade calculations
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
    """
    ids_trajectories = sorted(trajectory_history.items(), key=lambda x: x[0] in (track_ids or []))
    for track_id, trajectory in ids_trajectories:
        if len(trajectory) > 1:
            color = get_track_color(track_id, track_ids)
            
            # Calculate fade parameters
            max_fade_frames = 30  # Number of frames for complete fade after track ends
            current_time = frame_idx
            
            # Check if track is still active (within last 5 frames)
            track_is_active = trajectory and current_time - trajectory[-1][2] <= 5
            
            # Calculate fade alpha for the entire trajectory
            if track_is_active:
                # Track is active - full opacity
                alpha = 1.0
            else:
                # Track has ended - calculate fade based on time since last detection
                time_since_end = current_time - trajectory[-1][2]
                if time_since_end >= max_fade_frames:
                    alpha = 0.0  # Completely faded
                else:
                    alpha = 1.0 - (time_since_end / max_fade_frames)
            
            # Only draw if trajectory is still visible
            if alpha > 0.01:
                # Apply alpha to color for the entire trajectory
                line_color = tuple(int(c * alpha) for c in color)
                point_color = tuple(int(c * alpha) for c in color)
                
                # Draw trajectory lines
                for i in range(1, len(trajectory)):
                    prev_center = trajectory[i-1]
                    curr_center = trajectory[i]
                    
                    # Draw line
                    cv2.line(vis_frame, (prev_center[0], prev_center[1]), 
                             (curr_center[0], curr_center[1]), line_color, 2)
                    
                    # Draw trajectory points
                    point_radius = max(1, int(3 * alpha))
                    cv2.circle(vis_frame, (prev_center[0], prev_center[1]), point_radius, point_color, -1)
                
                # Draw final point
                final_center = trajectory[-1]
                cv2.circle(vis_frame, (final_center[0], final_center[1]), 3, point_color, -1)


def to_h264(input_path: str):
    """
    Convert video to H.264 codec with .h264 extension using FFMPEG.
    
    Args:
        input_path: Path to the input video file
    """
    # Create output path with .h264 extension
    # base_path = os.path.splitext(input_path)[0]
    output_path = f"{input_path[:-len('.mp4')]}.h264.mp4"
    
    # Run FFMPEG command to convert to H.264 (silent, optimized for small file size)
    cmd = [
        'ffmpeg', '-y',  # -y to overwrite output file
        '-loglevel', 'quiet',  # silence FFMPEG output
        '-i', input_path,  # input file
        '-c:v', 'libx264',  # H.264 codec
        '-preset', 'fast',  # encoding preset
        '-crf', '28',  # constant rate factor (lower quality, smaller file)
        '-profile:v', 'baseline',  # baseline profile for better compatibility
        '-level', '3.0',  # H.264 level for broader device support
        '-movflags', '+faststart',  # optimize for streaming/web playback
        '-pix_fmt', 'yuv420p',  # pixel format for maximum compatibility
        '-tune', 'fastdecode',  # optimize for faster decoding
        output_path
    ]
    
    subprocess.run(cmd, capture_output=True, text=True, check=True)


def create_tracking_visualization(video_path: str, tracking_results: dict[int, list[list[float]]], 
                                  output_path: str, speed_up: int, process_id: int, progress_queue=None, 
                                  track_ids: list[int] | None = None, detection_only: bool = False):
    """
    Create a visualization video showing tracking results overlaid on the original video.
    
    Args:
        video_path (str): Path to the input video file
        tracking_results (dict[int, list[list[float]]]): Tracking results from load_tracking_results
        output_path (str): Path where the output visualization video will be saved
        speed_up (int): Speed up factor for visualization (process every Nth frame)
        process_id (int): Process ID for logging
        progress_queue: Queue for progress updates
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, only show detections without trajectories, all boxes in green without track IDs
    """
    # print(f"Creating tracking visualization for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    assert cap.isOpened(), f"Could not open video {video_path}"
    
    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # print(f"Video info: {width}x{height}, {fps} FPS, {frame_count} frames")
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not writer.isOpened():
        print(f"Error: Could not create video writer for {output_path}")
        cap.release()
        return
    
    # print(f"Creating visualization video with {frame_count} frames at {fps} FPS")
    
    # Initialize trajectory history for all tracks with frame timestamps
    trajectory_history: dict[int, list[tuple[int, int, int]]] = {}  # track_id -> [(x, y, frame_idx), ...]
    
    # Initialize frame_idx for exception handling
    frame_idx = 0
    
    # Send initial progress update
    if progress_queue is not None:
        progress_queue.put((f'cuda:{process_id}', {
            'description': os.path.basename(video_path),
            'completed': 0,
            'total': frame_count
        }))
    
    # Process each frame
    for frame_idx in range(frame_count):
        # Read frame
        ret, frame = cap.read()
        if not ret:
            break
        
        # Get tracking results for this frame
        tracks = tracking_results.get(frame_idx, [])
        
        # Create visualization frame with trajectory history
        vis_frame = create_visualization_frame(frame, tracks, frame_idx, trajectory_history,
                                               speed_up, track_ids, detection_only)

        # Write frame to video
        if vis_frame is not None:
            writer.write(vis_frame)
        
        # Send progress update
        if progress_queue is not None:
            progress_queue.put((f'cuda:{process_id}', {'completed': frame_idx + 1}))
    
    cap.release()
    writer.release()

    to_h264(output_path)


def mark_detections(
    detections: list[list[float]],
    width: int,
    height: int,
    tilesize: int,
    detection_slice: slice = slice(-4, None)
) -> np.ndarray:
    """
    Mark tiles as relevant based on groundtruth detections.
    This function creates a bitmap where 1 indicates a tile with detection and 0 indicates no detection.
    
    Args:
        detections (list[list[float]]): List of bounding boxes, each formatted as [tracking_id, x1, y1, x2, y2]
        width (int): Frame width
        height (int): Frame height
        tilesize (int): Size of each tile
        detection_slice (slice): Slice of the bounding box to use for marking detections
        
    Returns:
        np.ndarray: 2D array representing the grid of tiles, where 1 indicates relevant tiles
    """
    bitmap = np.zeros((height // tilesize, width // tilesize), dtype=np.uint8)
    
    for bbox in detections:
        # Extract bounding box coordinates (ignore tracking_id)
        x1, y1, x2, y2 = bbox[detection_slice]  # Skip tracking_id at index 0
        
        # Convert to tile coordinates
        xfrom = int(max(0, x1) // tilesize)
        xto = int(min(width - 1, x2) // tilesize)
        yfrom = int(max(0, y1) // tilesize)
        yto = int(min(height - 1, y2) // tilesize)
        
        # Mark all tiles that overlap with the bounding box
        bitmap[yfrom:yto+1, xfrom:xto+1] = 1
    
    return bitmap


def create_timer(file: typing.TextIO, meta: dict | None = None):
    row = []
    def timer(op: str) -> Timer:
        return Timer(op, row)
    def flush():
        nonlocal row
        if row:
            file.write(json.dumps(row) + '\n')
            row = []
    return timer, flush


class Timer:
    def __init__(self, op: str, row: list[dict]):
        self.start_time = time.time_ns() / 1e6
        self.op = op
        self.row = row

    def __enter__(self):
        self.start_time = time.time_ns() / 1e6
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time_ns() / 1e6
        elapsed_time = self.end_time - self.start_time
        self.row.append({'op': self.op, 'time': elapsed_time})


def progress_bars(command_queue: "mp.Queue", num_workers: int, num_tasks: int,
                  refresh_per_second: float = 1):
    with progress.Progress(
        "[progress.description]{task.description}",
        progress.BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        # progress.TimeRemainingColumn(),
        progress.MofNCompleteColumn(),
        progress.TimeElapsedColumn(),
        refresh_per_second=refresh_per_second,
    ) as p:
        bars: dict[str, progress.TaskID] = {}
        overall_progress = p.add_task(f"[green]Processing {num_tasks} tasks",
                                      total=num_tasks, completed=-num_workers)
        bars['overall'] = overall_progress
        for gpu_id in range(num_workers):
            bars[f'cuda:{gpu_id}'] = p.add_task("")

        while True:
            val = command_queue.get()
            if val is None: break
            progress_id, kwargs = val
            # if kwargs.get('remove', False):
            #     p.remove_task(bars[progress_id])
            #     # bars.pop(progress_id)
            # else:
            p.update(bars[progress_id], **kwargs)
        
        # remove all tasks
        for _, task_id in bars.items():
            p.remove_task(task_id)
        bars.clear()


class ProgressBar:
    """
    Context manager for handling progress bars with multiprocessing support.
    
    Usage:
        with ProgressBar(num_workers=4, num_tasks=100) as pb:
            # Use pb.command_queue to send progress updates
            # Use pb.worker_id_queue to manage worker IDs
            pass
    """
    
    def __init__(self, num_workers: int, num_tasks: int, refresh_per_second: float = 1, off: bool = False):
        """
        Initialize the progress bar manager.
        
        Args:
            num_workers (int): Number of worker processes/GPUs
            num_tasks (int): Total number of tasks to process
            refresh_per_second (float): Refresh rate for progress bars
        """
        self.num_workers = min(num_workers, num_tasks)
        self.num_tasks = num_tasks
        self.refresh_per_second = refresh_per_second
        
        # Initialize queues
        self.command_queue: "mp.Queue[tuple[str, dict] | None]" = mp.Queue()
        self.worker_id_queue: "mp.Queue[int]" = mp.Queue(maxsize=num_workers)
        self.progress_process: mp.Process | None = None
        if not off:
            self.progress_process = mp.Process(
                target=progress_bars,
                args=(self.command_queue, self.num_workers,
                    self.num_tasks, self.refresh_per_second),
                daemon=True
            )
    
    def __enter__(self):
        """Enter the context manager - set up queues and start progress process."""

        # Populate worker ID queue
        for worker_id in range(self.num_workers):
            self.worker_id_queue.put(worker_id)
        
        # Start progress bars process
        if self.progress_process is not None:
            self.progress_process.start()
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the context manager - clean up progress bars and terminate process."""
        # Signal progress bars to stop
        self.command_queue.put(None)
        
        if self.progress_process is not None:
            # Wait for progress process to finish and terminate it
            self.progress_process.join(timeout=5)  # Wait up to 5 seconds
            if self.progress_process.is_alive():
                self.progress_process.terminate()
                self.progress_process.join(timeout=2)  # Give it time to terminate
            
            # Force kill if still alive
            if self.progress_process.is_alive():
                self.progress_process.kill()
                self.progress_process.join()
    
    def update_overall_progress(self, advance: int = 1):
        """Update the overall progress bar."""
        self.command_queue.put(('overall', {'advance': advance}))

    def get_worker_id(self):
        """Get a worker ID from the worker ID queue."""
        return self.worker_id_queue.get()
    
    def run(self, func: typing.Callable[[int, mp.Queue], None]):
        """Run func in a new process with a worker ID."""
        worker_id = self.worker_id_queue.get()
        self.update_overall_progress(1)
        process = mp.Process(target=ProgressBar.run_with_worker_id,
                             args=(func, worker_id, self.command_queue,
                                   self.worker_id_queue))
        process.start()
        return process
    
    def run_all(self, funcs: list[typing.Callable[[int, mp.Queue], None]]):
        """Run all funcs in a new process with a worker ID."""
        with self:
            processes: list[mp.Process] = []
            for func in funcs:
                processes.append(self.run(func))
            
            for _ in range(self.num_workers):
                worker_id = self.get_worker_id()
                self.update_overall_progress(1)
                self.command_queue.put((f'cuda:{worker_id}',
                                        {'remove': True}))

            for process in processes:
                process.join()
                process.terminate()


    @staticmethod
    def run_with_worker_id(func: typing.Callable[[int, mp.Queue], None],
                           worker_id: int, command_queue: mp.Queue,
                           worker_id_queue: mp.Queue):
        """Run func with a worker ID and command queue."""
        try: func(worker_id, command_queue)
        finally:
            kwargs = {'completed': 0, 'description': 'Done', 'total': 1}
            command_queue.put((f'cuda:{worker_id}', kwargs))
            worker_id_queue.put(worker_id)


def gcp_run(funcs: list[typing.Callable[[int, mp.Queue], None]]):
    """
    Run a list of functions in a GCP instance.
    
    Args:
        funcs (list[typing.Callable[[int, mp.Queue], None]]): List of functions to run
    """
    commands = []
    for func in funcs:
        assert isinstance(func, functools.partial)
        args: tuple = func.args
        func_name = func.func.__name__
        script: str = func.func.gcp  # type: ignore
        args_str = ' '.join(str(arg) for arg in args)
        command = f"python ./scripts/{script} {func_name} {args_str}"
        commands.append(command)
    
    command_funcs = [functools.partial(subprocess.run, command, shell=True, check=True, capture_output=True, text=True) for command in commands]
    processes = []
    for command_func in command_funcs:
        process = mp.Process(target=command_func)
        process.start()
        processes.append(process)
    
    for process in progress.track(processes):
        process.join()
        process.terminate()


def load_tradeoff_data(dataset: str):
    """
    Load pre-computed tradeoff data from CSV files created by p090_tradeoff_compute.py.
    
    Args:
        dataset: Dataset name
        csv_suffix: Suffix for CSV files ('runtime' or 'throughput')
        
    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: Individual and aggregated data DataFrames
    """
    # Construct paths to CSV files created by p090_tradeoff_compute.py
    tradeoff_dir = os.path.join(CACHE_DIR, dataset, 'evaluation', '090_tradeoff')
    
    tradeoff_path = os.path.join(tradeoff_dir, f'tradeoff.csv')
    tradeoff_combined_path = os.path.join(tradeoff_dir, f'tradeoff_combined.csv')
    naive_path = os.path.join(tradeoff_dir, f'naive.csv')
    naive_combined_path = os.path.join(tradeoff_dir, f'naive_combined.csv')
    
    # Check if CSV files exist
    assert os.path.exists(tradeoff_path), \
        f"Tradeoff data not found: {tradeoff_path}. " \
        "Please run p090_tradeoff_compute.py first."
    
    assert os.path.exists(tradeoff_combined_path), \
        f"Combined tradeoff data not found: {tradeoff_combined_path}. " \
        "Please run p090_tradeoff_compute.py first."
    
    # Load CSV files
    import pandas as pd
    tradeoff = pd.read_csv(tradeoff_path)
    combined = pd.read_csv(tradeoff_combined_path)
    naive = pd.read_csv(naive_path)
    naive_combined = pd.read_csv(naive_combined_path)
    
    print(f"Loaded individual tradeoff data: {len(tradeoff)} rows from {tradeoff_path}")
    print(f"Loaded combined tradeoff data: {len(combined)} rows from {tradeoff_combined_path}")
    
    return tradeoff, combined, naive, naive_combined


def load_all_datasets_tradeoff_data(datasets: list[str], system_name: str | None = None):
    """
    Load tradeoff data from all datasets and combine into a single DataFrame.

    Args:
        datasets: list of dataset names
        system_name: Optional system name to add as a column (e.g., 'Polytris')

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: Combined tradeoff data and naive data from all datasets
    """
    all_combined = []
    all_naive = []

    for dataset in datasets:
        # Use the load_tradeoff_data function
        _, combined, _, naive_combined = load_tradeoff_data(dataset)
        # Add dataset column to combined data
        combined['dataset'] = dataset
        naive_combined['dataset'] = dataset

        # Add system column if specified
        if system_name is not None:
            combined['system'] = system_name

        all_combined.append(combined)
        all_naive.append(naive_combined)

    # Combine all datasets
    import pandas as pd
    combined_df = pd.concat(all_combined, ignore_index=True)
    naive_df = pd.concat(all_naive, ignore_index=True)
    print(f"Combined tradeoff data from {len(datasets)} datasets: {len(combined_df)} total rows")

    return combined_df, naive_df


def print_best_data_points(df_combined: "pd.DataFrame", metrics_list: list[str],
                          x_column: str, plot_suffix: str, include_system: bool = False):
    """
    Print the best data point (highest accuracy, faster than baseline) for each dataset and metric as tables.

    Args:
        df_combined: Combined DataFrame with data from all datasets (already merged with naive data)
        metrics_list: list of metrics to analyze
        x_column: Column name for x-axis data (runtime or throughput)
        plot_suffix: Suffix for the analysis type ('runtime' or 'throughput')
        include_system: Whether to include the 'system' column in output (default: False)
    """
    import pandas as pd

    print(f"\n=== Best Data Points Analysis ({plot_suffix.upper()}) ===")

    # Naive column is automatically created from merge with suffix '_naive'
    naive_column = f'{x_column}_naive'

    for metric in metrics_list:
        if metric == 'HOTA':
            accuracy_col = 'HOTA_HOTA'
            metric_name = 'HOTA'
        elif metric == 'CLEAR':
            accuracy_col = 'MOTA_MOTA'
            metric_name = 'MOTA'
        else:
            continue

        print(f"\n--- {metric_name} Analysis ---")

        # Collect results for this metric
        results = []

        for dataset in df_combined['dataset'].unique():
            dataset_data = df_combined[df_combined['dataset'] == dataset]

            # Filter data points that are faster than baseline for this dataset
            faster_than_baseline = dataset_data[dataset_data[x_column] < dataset_data[naive_column]]

            if len(faster_than_baseline) == 0:
                # If no points are faster than baseline, use the fastest point
                assert isinstance(dataset_data, pd.DataFrame), \
                    f"dataset_data should be a DataFrame, got {type(dataset_data)}"
                best_point = dataset_data.loc[dataset_data[x_column].idxmin()]
            else:
                # Find the point with highest accuracy among those faster than baseline
                assert isinstance(faster_than_baseline, pd.DataFrame), \
                    f"faster_than_baseline should be a DataFrame, got {type(faster_than_baseline)}"
                best_point = faster_than_baseline.loc[faster_than_baseline[accuracy_col].idxmax()]

            # Calculate speed improvement
            naive_runtime = best_point[naive_column]
            best_runtime = best_point[x_column]
            speedup = naive_runtime / best_runtime if best_runtime > 0 else 0

            result = {
                'Dataset': dataset,
            }

            if include_system:
                result['System'] = best_point['system']

            result[f'{metric_name} Score'] = f"{best_point[accuracy_col]:.2f}"
            result['Speedup'] = f"{speedup:.2f}"

            results.append(result)

        # Create and print table for this metric
        if results:
            df = pd.DataFrame(results)
            print(df.to_string(index=False))
        else:
            print("No results found.")


def tradeoff_scatter_and_naive_baseline(base_chart: "alt.Chart", x_column: str, x_title: str, 
                                        accuracy_col: str, metric_name: str,
                                        size_range: tuple[int, int] = (20, 200), scatter_opacity: float = 0.7, 
                                        size: int | None = None, baseline_stroke_width: int = 2, 
                                        baseline_opacity: float = 0.8, size_field: str = 'tilesize') -> "tuple[alt.Chart, alt.LayerChart]":
    """
    Create both a scatter plot and naive baseline visualization with common styling.
    
    Args:
        base_chart: Base Altair chart
        x_column: Column name for x-axis data
        x_title: Title for x-axis
        accuracy_col: Column name for accuracy data
        metric_name: Name of the metric (e.g., 'HOTA', 'MOTA')
        naive_column: Column name for naive baseline data
        size_range: Tuple of (min, max) for tile size scale
        scatter_opacity: Opacity for the scatter points
        size: Fixed size for scatter points (if None, uses size_field encoding)
        baseline_stroke_width: Width of the baseline rule line
        baseline_opacity: Opacity of the baseline rule line
        size_field: Column name for size encoding (default: 'tilesize')
        
    Returns:
        tuple[alt.Chart, alt.Chart]: Tuple of (scatter_plot, naive_baseline)
    """
    import altair as alt
    # Create scatter plot
    scale = {'scale': alt.Scale(domain=[0, 1])} if metric_name != 'Count' else {}
    scatter = base_chart.mark_circle(opacity=scatter_opacity).encode(
        x=alt.X(f'{x_column}:Q', title=x_title),
        y=alt.Y(f'{accuracy_col}:Q', title=f'{metric_name} Score', **scale),
        color=alt.Color('classifier:N', title='Classifier'),
        tooltip=['video', 'classifier', size_field, x_column, accuracy_col]
    ).properties(
        width=150,
        height=150
    )
    
    # Add size encoding only if no fixed size is provided
    if size is None:
        scatter = scatter.encode(size=alt.Size(f'{size_field}:O',
                                 title='Tile Size',
                                 scale=alt.Scale(range=size_range)))
    
    # Create naive baseline as a point at 1.0 accuracy score
    baseline = base_chart.mark_point(
        color='red',
        fill='red',
        size=20,
        opacity=baseline_opacity,
    ).encode(
        x=f'{x_column}_naive:Q',
        y=alt.value(1.0)  # Fixed at 1.0 accuracy score
    )
    
    # Create annotation text for the baseline
    baseline_annotation = base_chart.mark_text(
        align='right',
        baseline='top',
        fontSize=12,
        fontWeight='bold',
        color='red',
        dy=3,
        dx=15,
        lineHeight=10
    ).encode(
        x=f'{x_column}_naive:Q',
        y=alt.value(1.0),  # Position at 1.0 accuracy score
        text=alt.value(['Without', 'Optimization'])
    )
    
    return scatter, baseline + baseline_annotation


STR_NA = '_NA_'
INT_NA = 0


OPTIMAL_PARAMS = {
    'jnc0': {
        'classifier': 'YoloN',
        'tilesize': 60,
        'tilepadding': 'unpadded',
    },
    'jnc2': {
        'classifier': 'YoloN',
        'tilesize': 60,
        'tilepadding': 'unpadded',
    },
    'jnc6': {
        'classifier': 'YoloN',
        'tilesize': 60,
        'tilepadding': 'unpadded',
    },
    'jnc7': {
        'classifier': 'YoloN',
        'tilesize': 60,
        'tilepadding': 'unpadded',
    },
    'caldot1': {
        'classifier': 'ShuffleNet05',
        'tilesize': 60,
        'tilepadding': 'padded',
    },
    'caldot2': {
        'classifier': 'MobileNetS',
        'tilesize': 60,
        'tilepadding': 'padded',
    },
}

CHOSEN_PARAMS = {
    'jnc0': [
        {'classifier': 'YoloN', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
    'jnc2': [
        {'classifier': 'YoloN', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
    'jnc6': [
        {'classifier': 'YoloN', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
    'jnc7': [
        {'classifier': 'YoloN', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'unpadded'},
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
    'caldot1': [
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'padded'},
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'padded'},
        {'classifier': 'YoloN', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
    'caldot2': [
        {'classifier': 'MobileNetS', 'tilesize': 60, 'tilepadding': 'padded'},
        {'classifier': 'ShuffleNet05', 'tilesize': 60, 'tilepadding': 'unpadded'},
    ],
}


VIDEO_SETS = ['train', 'valid', 'test']
PREFIX_TO_VIDEOSET = {
    'tr': 'train',
    'va': 'valid',
    'te': 'test',
}


PARAMS = [
    'classifier',
    'tilesize',
    'tilepadding',
]

TilePadding = typing.Literal['none', 'connected', 'disconnected']
TILEPADDING_MODES: "dict[TilePadding, int]" = {
    'none': 0,
    'connected': 1,
    'disconnected': 2,
}

ParamTypes = tuple[str, int, str]


METRICS = [
    'HOTA',
    # 'CLEAR',
    # 'Identity',
    'Count',
]


DATASETS_TO_TEST = [
    'jnc0',
    'jnc2',
    'jnc6',
    'jnc7',
    # 'caldot1-yolov5',
    # 'caldot2-yolov5',
    'caldot1',
    'caldot2',
]


CLASSIFIERS_TO_TEST = [
    # 'SimpleCNN',
    'YoloN',
    # 'YoloS',
    # 'YoloM',
    # 'YoloL',
    # 'YoloX',
    'ShuffleNet05',
    # 'ShuffleNet20',
    # 'MobileNetL',
    'MobileNetS',
    # 'WideResNet50',
    # 'WideResNet101',
    # 'ResNet18', 
    # 'ResNet101',
    # 'ResNet152',
    # 'EfficientNetS',
    # 'EfficientNetL',
]

CLASSIFIERS = CLASSIFIERS_TO_TEST + ['Perfect']

CLASSIFIERS_CHOICES = [
    # Cutsom CNNs
    'SimpleCNN',

    # YOLOv11 models
    'YoloN',
    'YoloS',
    'YoloM',
    'YoloL',
    'YoloX',

    # ShuffleNet models
    'ShuffleNet05',
    'ShuffleNet20',

    # MobileNet models
    'MobileNetL',
    'MobileNetS',

    # ResNet models
    'ResNet18',
    'ResNet101',
    'ResNet152',

    # WideResNet models
    'WideResNet50',
    'WideResNet101',

    # EfficientNet models
    'EfficientNetS',
    'EfficientNetL',
]


def get_classifier_from_name(classifier_name: str):
    """
    Get the classifier class based on the classifier name.
    
    Args:
        classifier_name (str): Name of the classifier to use
        
    Returns:
        The classifier class
        
    Raises:
        ValueError: If the classifier is not supported
    """
    if classifier_name == 'SimpleCNN':
        from polyis.models.classifier.simple_cnn import SimpleCNN
        return SimpleCNN
    elif classifier_name == 'YoloN':
        from polyis.models.classifier.yolo import YoloN
        return YoloN
    elif classifier_name == 'YoloS':
        from polyis.models.classifier.yolo import YoloS
        return YoloS
    elif classifier_name == 'YoloM':
        from polyis.models.classifier.yolo import YoloM
        return YoloM
    elif classifier_name == 'YoloL':
        from polyis.models.classifier.yolo import YoloL
        return YoloL
    elif classifier_name == 'YoloX':
        from polyis.models.classifier.yolo import YoloX
        return YoloX
    elif classifier_name == 'ShuffleNet05':
        from polyis.models.classifier.shufflenet import ShuffleNet05
        return ShuffleNet05
    elif classifier_name == 'ShuffleNet20':
        from polyis.models.classifier.shufflenet import ShuffleNet20
        return ShuffleNet20
    elif classifier_name == 'MobileNetL':
        from polyis.models.classifier.mobilenet import MobileNetL
        return MobileNetL
    elif classifier_name == 'MobileNetS':
        from polyis.models.classifier.mobilenet import MobileNetS
        return MobileNetS
    elif classifier_name == 'WideResNet50':
        from polyis.models.classifier.wide_resnet import WideResNet50
        return WideResNet50
    elif classifier_name == 'WideResNet101':
        from polyis.models.classifier.wide_resnet import WideResNet101
        return WideResNet101
    elif classifier_name == 'ResNet152':
        from polyis.models.classifier.resnet import ResNet152
        return ResNet152
    elif classifier_name == 'ResNet101':
        from polyis.models.classifier.resnet import ResNet101
        return ResNet101
    elif classifier_name == 'ResNet18':
        from polyis.models.classifier.resnet import ResNet18
        return ResNet18
    elif classifier_name == 'EfficientNetS':
        from polyis.models.classifier.efficientnet import EfficientNetS
        return EfficientNetS
    elif classifier_name == 'EfficientNetL':
        from polyis.models.classifier.efficientnet import EfficientNetL
        return EfficientNetL
    else:
        raise ValueError(f"Unsupported classifier: {classifier_name}")


class FakeQueue(queue.Queue):
    def __init__(self):
        pass

    def put(self, item, block: bool = True, timeout: float | None = None):
        pass


def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True