import json
import os
import subprocess
import typing
import multiprocessing as mp

import cv2
import numpy as np
from rich import progress

if typing.TYPE_CHECKING:
    import altair as alt
    import pandas as pd

DATA_RAW_DIR = '/polyis-data/video-datasets-raw'
DATA_DIR = '/polyis-data/video-datasets-low'
CACHE_DIR = '/polyis-cache'

# Define 10 distinct colors for track visualization (BGR format for OpenCV)
TRACK_COLORS = [
    (255, 0, 0),    # Blue
    (0, 255, 0),    # Green
    (0, 0, 255),    # Red
    (255, 255, 0),  # Cyan
    (255, 0, 255),  # Magenta
    (0, 255, 255),  # Yellow
    (128, 0, 255),  # Purple
    (255, 128, 0),  # Orange
    (0, 128, 255),  # Light Blue
    (255, 0, 128),  # Pink
]


def format_time(**kwargs: float | int) -> list[dict[str, float | int | str]]:
    """
    Format timing information into a list of dictionaries.
    
    Args:
        **kwargs: Keyword arguments where keys are operation names and values are timing values
        
    Returns:
        list: List of dictionaries with 'op' (operation) and 'time' keys for each input argument
        
    Example:
        >>> format_time(read=1.5, detect=2.3)
        [{'op': 'read', 'time': 1.5}, {'op': 'detect', 'time': 2.3}]
    """
    return [{'op': op, 'time': time} for op, time in kwargs.items()]


def load_detection_results(cache_dir: str, dataset: str, video_file: str, tracking: bool = False, verbose: bool = False) -> list[dict]:
    """
    Load detection results from the JSONL file generated by 001_preprocess_groundtruth_detection.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tracking (bool): Whether to load tracking results instead of detection results
        verbose (bool): Whether to print verbose output
    Returns:
        list[dict]: list of frame detection results
        
    Raises:
        FileNotFoundError: If no detection results file is found
    """
    file = 'tracking.jsonl' if tracking else 'detections.jsonl'
    detection_path = os.path.join(cache_dir, dataset, 'execution', video_file, '000_groundtruth', file)
    
    if not os.path.exists(detection_path):
        raise FileNotFoundError(f"Detection results not found: {detection_path}")
    
    if verbose:
        print(f"Loading detection results from: {detection_path}")
    
    results = []
    with open(detection_path, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    if verbose:
        print(f"Loaded {len(results)} frame detections")
    return results


def load_tracking_results(cache_dir: str, dataset: str, video_file: str, verbose: bool = False) -> dict[int, list[list[float]]]:
    """
    Load tracking results from the JSONL file generated by 002_preprocess_groundtruth_tracking.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        verbose (bool): Whether to print verbose output
    Returns:
        dict[int, list[list[float]]]: dictionary mapping frame indices to lists of tracks
        
    Raises:
        FileNotFoundError: If no tracking results file is found
    """
    tracking_path = os.path.join(cache_dir, dataset, 'execution', video_file, '000_groundtruth', 'tracking.jsonl')
    
    if not os.path.exists(tracking_path):
        raise FileNotFoundError(f"Tracking results not found: {tracking_path}")
    
    if verbose:
        print(f"Loading tracking results from: {tracking_path}")
    
    frame_tracks = {}
    with open(tracking_path, 'r') as f:
        for line in f:
            if line.strip():
                frame_data = json.loads(line)
                frame_idx = frame_data['frame_idx']
                tracks = frame_data['tracks']
                frame_tracks[frame_idx] = tracks
    
    if verbose:
        print(f"Loaded tracking results for {len(frame_tracks)} frames")
    return frame_tracks


def interpolate_trajectory(trajectory: list[tuple[int, np.ndarray]], nxt: tuple[int, np.ndarray]) -> list[tuple[int, np.ndarray]]:
    """
    Perform linear interpolation between two trajectory points except the last point (nxt).
    
    Args:
        trajectory (list[tuple[int, np.ndarray]]): list of (frame_idx, detection) tuples
        nxt (tuple[int, np.ndarray]): Next detection point (frame_idx, detection)
        
    Returns:
        list[tuple[int, np.ndarray]]: list of interpolated points
    """
    extend: list[tuple[int, np.ndarray]] = []
    
    if len(trajectory) != 0:
        prv = trajectory[-1]
        assert prv[0] < nxt[0]
        prv_det = prv[1]
        nxt_det = nxt[1]
        dif_det = nxt_det - prv_det
        dif_det = dif_det.reshape(1, -1)

        scale = np.arange(0, nxt[0] - prv[0], dtype=np.float32).reshape(-1, 1) / (nxt[0] - prv[0])
        
        int_dets = (scale @ dif_det) + prv_det.reshape(1, -1)

        for idx, int_det in enumerate(int_dets[:-1]):
            extend.append((prv[0] + idx + 1, int_det))

    return extend


def register_tracked_detections(
    tracked_dets: np.ndarray,
    frame_idx: int,
    frame_tracks: dict[int, list[list[float]]],
    trajectories: dict[int, list[tuple[int, np.ndarray]]],
    no_interpolate: bool
):
    """
    Register tracked detections to frame tracks and trajectories.

    Args:
        tracked_dets (np.ndarray): Tracked detections
        frame_idx (int): Frame index
        frame_tracks (dict[int, list[list[float]]]): Frame tracks
        trajectories (dict[int, list[tuple[int, np.ndarray]]]): Trajectories
        no_interpolate (bool): Whether to not perform trajectory interpolation
    """

    if tracked_dets.size == 0:
        return

    if frame_idx not in frame_tracks:
        frame_tracks[frame_idx] = []

    for track in tracked_dets:
        # SORT returns: [x1, y1, x2, y2, track_id]
        x1, y1, x2, y2, track_id = track
        track_id = int(track_id)
        
        # # Convert to detection format: [track_id, x1, y1, x2, y2]
        # detection = [track_id, x1, y1, x2, y2]
        
        # # Add to frame tracks
        # if frame_idx not in frame_tracks:
        #     frame_tracks[frame_idx] = []
        # frame_tracks[frame_idx].append(detection)

        if track_id not in trajectories:
            trajectories[track_id] = []
        box_array = np.array([x1, y1, x2, y2], dtype=np.float16)
        
        # Add to trajectories for interpolation (if enabled)
        if no_interpolate:
            continue

        extend = interpolate_trajectory(trajectories[track_id],
                                        (frame_idx, box_array))
        
        # Add interpolated points to frame tracks
        for e in extend + [(frame_idx, box_array)]:
            e_frame_idx, e_box = e
            if e_frame_idx not in frame_tracks:
                frame_tracks[e_frame_idx] = []
            
            # Convert back to list format: [track_id, x1, y1, x2, y2]
            e_detection = [track_id, *e_box.tolist()]
            frame_tracks[e_frame_idx].append(e_detection)

            # Add interpolated points to trajectories
            trajectories[track_id].append((e_frame_idx, e_box))

        # trajectories[track_id].append((frame_idx, box_array))


def get_track_color(track_id: int, track_ids: list[int] | None = None) -> tuple[int, int, int]:
    """
    Get a color for a track ID by cycling through the predefined colors.
    If track_ids is specified, only those track IDs get colors, others get grey.
    
    Args:
        track_id (int): Track ID
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        
    Returns:
        tuple[int, int, int]: BGR color tuple
    """
    # If track_ids is specified and this track_id is not in the list, return grey
    if track_ids is not None and track_id not in track_ids:
        return (128, 128, 128)  # Grey color in BGR format
    
    # Otherwise, use the normal color cycling
    color_index = track_id % len(TRACK_COLORS)
    return TRACK_COLORS[color_index]


def overlapi(interval1: tuple[int, int], interval2: tuple[int, int]):
    """
    Check if two 1D intervals overlap.
    
    Args:
        interval1 (tuple[int, int]): First interval as (start, end)
        interval2 (tuple[int, int]): Second interval as (start, end)
        
    Returns:
        bool: True if the intervals overlap, False otherwise
    """
    return (
        (interval1[0] <= interval2[0] <= interval1[1]) or
        (interval1[0] <= interval2[1] <= interval1[1]) or
        (interval2[0] <= interval1[0] <= interval2[1]) or
        (interval2[0] <= interval1[1] <= interval2[1])
    )


def overlap(b1, b2):
    """
    Check if two 2D bounding boxes overlap.
    
    Args:
        b1: First bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        b2: Second bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        
    Returns:
        bool: True if the bounding boxes overlap in both x and y dimensions, False otherwise
    """
    return overlapi((b1[0], b1[2]), (b2[0], b2[2])) and overlapi((b1[1], b1[3]), (b2[1], b2[3]))


def get_precision(tp: int, fp: int) -> float:
    """
    Calculate precision.
    
    Args:
        tp (int): True positives
        fp (int): False positives
    """
    if (tp + fp) == 0:
        return 0.0
    return tp / (tp + fp)

def get_recall(tp: int, fn: int) -> float:
    """
    Calculate recall.
    
    Args:
        tp (int): True positives
        fn (int): False negatives
    """
    if (tp + fn) == 0:
        return 0.0
    return tp / (tp + fn)

def get_accuracy(tp: int, tn: int, fp: int, fn: int) -> float:

    """
    Calculate accuracy.
    
    Args:
        tp (int): True positives
        tn (int): True negatives
        fp (int): False positives
        fn (int): False negatives
    """
    if (tp + tn + fp + fn) == 0:
        return 0.0
    return (tp + tn) / (tp + tn + fp + fn)

def get_f1_score(tp: int, fp: int, fn: int) -> float:

    """
    Calculate F1 score.
    
    Args:
        tp (int): True positives
        fp (int): False positives
        fn (int): False negatives
    """
    if (get_precision(tp, fp) + get_recall(tp, fn)) == 0:
        return 0.0
    precision = get_precision(tp, fp)
    recall = get_recall(tp, fn)
    return 2. * (precision * recall) / (precision + recall)


def load_classification_results(cache_dir: str, dataset: str, video_file: str,
                                tile_size: int, classifier: str, verbose: bool = False,
                                execution_dir: bool = False) -> list:
    """
    Load classification results from the JSONL file generated by 020_exec_classify.py or 021_exec_classify_correct.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for classification
        classifier (str): Classifier name to use
        verbose (bool): Whether to print verbose output
        execution_dir (bool): Whether to look in execution directory (for 021_exec_classify_correct.py results)
        
    Returns:
        list: List of frame classification results, each containing frame data and classifications
        
    Raises:
        FileNotFoundError: If no classification results file is found
    """
    # Look for the classification results file
    if execution_dir:
        score_dir = os.path.join(cache_dir, dataset, 'execution', video_file, '020_relevancy', f'{classifier}_{tile_size}', 'score')
    else:
        score_dir = os.path.join(cache_dir, dataset, video_file, '020_relevancy', f'{classifier}_{tile_size}', 'score')
    
    # Use model scores
    expected_filename = 'score.jsonl'
    
    # Look for the specific results file
    results_file = os.path.join(score_dir, expected_filename)
    
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Classification results file not found: {results_file}")
    
    if verbose:
        print(f"Loading classification results from: {results_file}")
    
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    if verbose:
        print(f"Loaded {len(results)} frame classifications")
    return results


def create_tracker(tracker_name: str, max_age: int = 1, min_hits: int = 3, iou_threshold: float = 0.3):
    """
    Create a tracker instance based on the specified algorithm.
    
    Args:
        tracker_name (str): Name of the tracking algorithm
        max_age (int): Maximum age for SORT tracker
        min_hits (int): Minimum hits for SORT tracker
        iou_threshold (float): IOU threshold for SORT tracker
        
    Returns:
        Tracker instance
        
    Raises:
        ValueError: If the tracker name is not supported
    """
    if tracker_name == 'sort':
        # print(f"Creating SORT tracker with max_age={max_age}, min_hits={min_hits}, iou_threshold={iou_threshold}")
        from modules.b3d.b3d.external.sort import Sort
        return Sort(max_age=max_age, min_hits=min_hits, iou_threshold=iou_threshold)
    else:
        raise ValueError(f"Unknown tracker: {tracker_name}")


def create_visualization_frame(frame: np.ndarray, tracks: list[list[float]], frame_idx: int,
                               trajectory_history: dict[int, list[tuple[int, int, int]]], 
                               speed_up: int, track_ids: list[int] | None, detection_only: bool = False) -> np.ndarray | None:
    """
    Create a visualization frame by drawing bounding boxes and trajectories for all tracks.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        tracks (list[list[float]]): list of tracks for this frame
        frame_idx (int): Frame index for logging
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        speed_up (int): Speed up factor (process every Nth frame)
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, only show detections without trajectories, all boxes in green without track IDs
        
    Returns:
        np.ndarray | None: Frame with bounding boxes and trajectories drawn, or None if frame should be skipped
    """
    # First loop: Update trajectory history for all tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            track_id = int(track_id)
            
            # Calculate center of bounding box
            center_x = int((x1 + x2) // 2)
            center_y = int((y1 + y2) // 2)
            
            # Update trajectory history with frame timestamp
            if track_id not in trajectory_history:
                trajectory_history[track_id] = []
            trajectory_history[track_id].append((center_x, center_y, frame_idx))

    if frame_idx % speed_up != 0:
        return None
    
    # Create a copy of the frame for visualization
    vis_frame = frame.copy()
    
    if detection_only:
        # In detection-only mode, draw all bounding boxes in green without track IDs
        draw_track_bounding_boxes(vis_frame, tracks, track_ids, detection_only=True)
    else:
        # Draw bounding boxes and labels for tracks not in track_ids (grey)
        tracks_not_in_track_ids = [track for track in tracks if track[0] not in (track_ids or [])]
        draw_track_bounding_boxes(vis_frame, tracks_not_in_track_ids, track_ids)
        
        # Draw all trajectories with gradual fading
        draw_trajectories(vis_frame, trajectory_history, frame_idx, track_ids)
        
        # Draw bounding boxes and labels for tracks in track_ids (colored)
        tracks_in_track_ids = [track for track in tracks if track[0] in (track_ids or [])]
        draw_track_bounding_boxes(vis_frame, tracks_in_track_ids, track_ids)
    
    return vis_frame


def draw_track_bounding_boxes(vis_frame: np.ndarray, tracks: list[list[float]], 
                              track_ids: list[int] | None, detection_only: bool = False):
    """
    Draw bounding boxes and labels for all tracks on the visualization frame.
    
    Args:
        vis_frame (np.ndarray): Frame to draw on (modified in place)
        tracks (list[list[float]]): List of tracks for this frame
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, use green color for all boxes and hide track IDs
    """
    for track in tracks:
        assert len(track) >= 5, f"Track must have at least 5 elements: {track}"
        track_id, x1, y1, x2, y2 = track[:5]
        
        # Convert to integers for drawing
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        track_id = int(track_id)
        
        color = (0, 255, 0) if detection_only else get_track_color(track_id, track_ids)
        
        # Draw bounding box
        cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)
        
        # Draw track ID label only if not in detection-only mode
        if detection_only:
            continue

        label = str(track_id)
        font_scale = 0.6
        font_thickness = 2
        
        # Calculate text size and position
        (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                                                font_scale, font_thickness)
        
        # Position text above the bounding box
        text_x = x1
        text_y = max(y1 - 10, text_height + 5)
        
        # Draw text background for better visibility
        cv2.rectangle(vis_frame, (text_x - 2, text_y - text_height - 2), 
                        (text_x + text_width + 2, text_y + baseline + 2), 
                        color, -1)
        
        # Draw text
        cv2.putText(vis_frame, label, (text_x, text_y), 
                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)


def draw_trajectories(vis_frame: np.ndarray, trajectory_history: dict[int, list[tuple[int, int, int]]], 
                      frame_idx: int, track_ids: list[int] | None):
    """
    Draw all trajectories with gradual fading on the visualization frame.
    
    Args:
        vis_frame (np.ndarray): Frame to draw on (modified in place)
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        frame_idx (int): Current frame index for fade calculations
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
    """
    ids_trajectories = sorted(trajectory_history.items(), key=lambda x: x[0] in (track_ids or []))
    for track_id, trajectory in ids_trajectories:
        if len(trajectory) > 1:
            color = get_track_color(track_id, track_ids)
            
            # Calculate fade parameters
            max_fade_frames = 30  # Number of frames for complete fade after track ends
            current_time = frame_idx
            
            # Check if track is still active (within last 5 frames)
            track_is_active = trajectory and current_time - trajectory[-1][2] <= 5
            
            # Calculate fade alpha for the entire trajectory
            if track_is_active:
                # Track is active - full opacity
                alpha = 1.0
            else:
                # Track has ended - calculate fade based on time since last detection
                time_since_end = current_time - trajectory[-1][2]
                if time_since_end >= max_fade_frames:
                    alpha = 0.0  # Completely faded
                else:
                    alpha = 1.0 - (time_since_end / max_fade_frames)
            
            # Only draw if trajectory is still visible
            if alpha > 0.01:
                # Apply alpha to color for the entire trajectory
                line_color = tuple(int(c * alpha) for c in color)
                point_color = tuple(int(c * alpha) for c in color)
                
                # Draw trajectory lines
                for i in range(1, len(trajectory)):
                    prev_center = trajectory[i-1]
                    curr_center = trajectory[i]
                    
                    # Draw line
                    cv2.line(vis_frame, (prev_center[0], prev_center[1]), 
                             (curr_center[0], curr_center[1]), line_color, 2)
                    
                    # Draw trajectory points
                    point_radius = max(1, int(3 * alpha))
                    cv2.circle(vis_frame, (prev_center[0], prev_center[1]), point_radius, point_color, -1)
                
                # Draw final point
                final_center = trajectory[-1]
                cv2.circle(vis_frame, (final_center[0], final_center[1]), 3, point_color, -1)


def to_h264(input_path: str):
    """
    Convert video to H.264 codec with .h264 extension using FFMPEG.
    
    Args:
        input_path: Path to the input video file
    """
    # Create output path with .h264 extension
    # base_path = os.path.splitext(input_path)[0]
    output_path = f"{input_path[:-len('.mp4')]}.h264.mp4"
    
    # Run FFMPEG command to convert to H.264 (silent, optimized for small file size)
    cmd = [
        'ffmpeg', '-y',  # -y to overwrite output file
        '-loglevel', 'quiet',  # silence FFMPEG output
        '-i', input_path,  # input file
        '-c:v', 'libx264',  # H.264 codec
        '-preset', 'fast',  # encoding preset
        '-crf', '28',  # constant rate factor (lower quality, smaller file)
        '-profile:v', 'baseline',  # baseline profile for better compatibility
        '-level', '3.0',  # H.264 level for broader device support
        '-movflags', '+faststart',  # optimize for streaming/web playback
        '-pix_fmt', 'yuv420p',  # pixel format for maximum compatibility
        '-tune', 'fastdecode',  # optimize for faster decoding
        output_path
    ]
    
    subprocess.run(cmd, capture_output=True, text=True, check=True)


def create_tracking_visualization(video_path: str, tracking_results: dict[int, list[list[float]]], 
                                  output_path: str, speed_up: int, process_id: int, progress_queue=None, 
                                  track_ids: list[int] | None = None, detection_only: bool = False):
    """
    Create a visualization video showing tracking results overlaid on the original video.
    
    Args:
        video_path (str): Path to the input video file
        tracking_results (dict[int, list[list[float]]]): Tracking results from load_tracking_results
        output_path (str): Path where the output visualization video will be saved
        speed_up (int): Speed up factor for visualization (process every Nth frame)
        process_id (int): Process ID for logging
        progress_queue: Queue for progress updates
        track_ids (list[int] | None): List of track IDs to color (others will be grey)
        detection_only (bool): If True, only show detections without trajectories, all boxes in green without track IDs
    """
    # print(f"Creating tracking visualization for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    assert cap.isOpened(), f"Could not open video {video_path}"
    
    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # print(f"Video info: {width}x{height}, {fps} FPS, {frame_count} frames")
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not writer.isOpened():
        print(f"Error: Could not create video writer for {output_path}")
        cap.release()
        return
    
    # print(f"Creating visualization video with {frame_count} frames at {fps} FPS")
    
    # Initialize trajectory history for all tracks with frame timestamps
    trajectory_history: dict[int, list[tuple[int, int, int]]] = {}  # track_id -> [(x, y, frame_idx), ...]
    
    # Initialize frame_idx for exception handling
    frame_idx = 0
    
    # Send initial progress update
    if progress_queue is not None:
        progress_queue.put((f'cuda:{process_id}', {
            'description': os.path.basename(video_path),
            'completed': 0,
            'total': frame_count
        }))
    
    # Process each frame
    for frame_idx in range(frame_count):
        # Read frame
        ret, frame = cap.read()
        if not ret:
            break
        
        # Get tracking results for this frame
        tracks = tracking_results.get(frame_idx, [])
        
        # Create visualization frame with trajectory history
        vis_frame = create_visualization_frame(frame, tracks, frame_idx, trajectory_history,
                                               speed_up, track_ids, detection_only)

        # Write frame to video
        if vis_frame is not None:
            writer.write(vis_frame)
        
        # Send progress update
        if progress_queue is not None:
            progress_queue.put((f'cuda:{process_id}', {'completed': frame_idx + 1}))
    
    cap.release()
    writer.release()

    to_h264(output_path)


def mark_detections(
    detections: list[list[float]],
    width: int,
    height: int,
    tile_size: int,
    detection_slice: slice = slice(-4, None)
) -> np.ndarray:
    """
    Mark tiles as relevant based on groundtruth detections.
    This function creates a bitmap where 1 indicates a tile with detection and 0 indicates no detection.
    
    Args:
        detections (list[list[float]]): List of bounding boxes, each formatted as [tracking_id, x1, y1, x2, y2]
        width (int): Frame width
        height (int): Frame height
        tile_size (int): Size of each tile
        detection_slice (slice): Slice of the bounding box to use for marking detections
        
    Returns:
        np.ndarray: 2D array representing the grid of tiles, where 1 indicates relevant tiles
    """
    bitmap = np.zeros((height // tile_size, width // tile_size), dtype=np.uint8)
    
    for bbox in detections:
        # Extract bounding box coordinates (ignore tracking_id)
        x1, y1, x2, y2 = bbox[detection_slice]  # Skip tracking_id at index 0
        
        # Convert to tile coordinates
        xfrom = int(max(0, x1) // tile_size)
        xto = int(min(width - 1, x2) // tile_size)
        yfrom = int(max(0, y1) // tile_size)
        yto = int(min(height - 1, y2) // tile_size)
        
        # Mark all tiles that overlap with the bounding box
        bitmap[yfrom:yto+1, xfrom:xto+1] = 1
    
    return bitmap


def progress_bars(command_queue: "mp.Queue", num_workers: int, num_tasks: int,
                  refresh_per_second: float = 1):
    with progress.Progress(
        "[progress.description]{task.description}",
        progress.BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        # progress.TimeRemainingColumn(),
        progress.MofNCompleteColumn(),
        progress.TimeElapsedColumn(),
        refresh_per_second=refresh_per_second,
    ) as p:
        bars: dict[str, progress.TaskID] = {}
        overall_progress = p.add_task(f"[green]Processing {num_tasks} tasks",
                                      total=num_tasks, completed=-num_workers)
        bars['overall'] = overall_progress
        for gpu_id in range(num_workers):
            bars[f'cuda:{gpu_id}'] = p.add_task("video tilesize model T/V")

        while True:
            val = command_queue.get()
            if val is None: break
            progress_id, kwargs = val
            # if kwargs.get('remove', False):
            #     p.remove_task(bars[progress_id])
            #     # bars.pop(progress_id)
            # else:
            p.update(bars[progress_id], **kwargs)
        
        # remove all tasks
        for _, task_id in bars.items():
            p.remove_task(task_id)
        bars.clear()


class ProgressBar:
    """
    Context manager for handling progress bars with multiprocessing support.
    
    Usage:
        with ProgressBar(num_workers=4, num_tasks=100) as pb:
            # Use pb.command_queue to send progress updates
            # Use pb.worker_id_queue to manage worker IDs
            pass
    """
    
    def __init__(self, num_workers: int, num_tasks: int, refresh_per_second: float = 1, off: bool = False):
        """
        Initialize the progress bar manager.
        
        Args:
            num_workers (int): Number of worker processes/GPUs
            num_tasks (int): Total number of tasks to process
            refresh_per_second (float): Refresh rate for progress bars
        """
        self.num_workers = min(num_workers, num_tasks)
        self.num_tasks = num_tasks
        self.refresh_per_second = refresh_per_second
        
        # Initialize queues
        self.command_queue: "mp.Queue[tuple[str, dict] | None]" = mp.Queue()
        self.worker_id_queue: "mp.Queue[int]" = mp.Queue(maxsize=num_workers)
        self.progress_process: mp.Process | None = None
        if not off:
            self.progress_process = mp.Process(
                target=progress_bars,
                args=(self.command_queue, self.num_workers,
                    self.num_tasks, self.refresh_per_second),
                daemon=True
            )
    
    def __enter__(self):
        """Enter the context manager - set up queues and start progress process."""

        # Populate worker ID queue
        for worker_id in range(self.num_workers):
            self.worker_id_queue.put(worker_id)
        
        # Start progress bars process
        if self.progress_process is not None:
            self.progress_process.start()
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the context manager - clean up progress bars and terminate process."""
        # Signal progress bars to stop
        self.command_queue.put(None)
        
        if self.progress_process is not None:
            # Wait for progress process to finish and terminate it
            self.progress_process.join(timeout=5)  # Wait up to 5 seconds
            if self.progress_process.is_alive():
                self.progress_process.terminate()
                self.progress_process.join(timeout=2)  # Give it time to terminate
            
            # Force kill if still alive
            if self.progress_process.is_alive():
                self.progress_process.kill()
                self.progress_process.join()
    
    def update_overall_progress(self, advance: int = 1):
        """Update the overall progress bar."""
        self.command_queue.put(('overall', {'advance': advance}))

    def get_worker_id(self):
        """Get a worker ID from the worker ID queue."""
        return self.worker_id_queue.get()
    
    def run(self, func: typing.Callable[[int, mp.Queue], None]):
        """Run func in a new process with a worker ID."""
        worker_id = self.worker_id_queue.get()
        self.update_overall_progress(1)
        process = mp.Process(target=ProgressBar.run_with_worker_id,
                             args=(func, worker_id, self.command_queue,
                                   self.worker_id_queue))
        process.start()
        return process
    
    def run_all(self, funcs: list[typing.Callable[[int, mp.Queue], None]]):
        """Run all funcs in a new process with a worker ID."""
        with self:
            processes: list[mp.Process] = []
            for func in funcs:
                processes.append(self.run(func))
            
            for _ in range(self.num_workers):
                worker_id = self.get_worker_id()
                self.update_overall_progress(1)
                self.command_queue.put((f'cuda:{worker_id}',
                                        {'remove': True}))

            for process in processes:
                process.join()
                process.terminate()

    @staticmethod
    def run_with_worker_id(func: typing.Callable[[int, mp.Queue], None],
                           worker_id: int, command_queue: mp.Queue,
                           worker_id_queue: mp.Queue):
        """Run func with a worker ID and command queue."""
        try: func(worker_id, command_queue)
        finally:
            kwargs = {'completed': 0, 'description': 'Done', 'total': 1}
            command_queue.put((f'cuda:{worker_id}', kwargs))
            worker_id_queue.put(worker_id)


def load_tradeoff_data(dataset: str, csv_suffix: str) -> "tuple[pd.DataFrame, pd.DataFrame]":
    """
    Load pre-computed tradeoff data from CSV files created by p090_tradeoff_compute.py.
    
    Args:
        dataset: Dataset name
        csv_suffix: Suffix for CSV files ('runtime' or 'throughput')
        
    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: Individual and aggregated data DataFrames
    """
    # Construct paths to CSV files created by p090_tradeoff_compute.py
    tradeoff_dir = os.path.join(CACHE_DIR, dataset, 'evaluation', '090_tradeoff')
    
    individual_csv_path = os.path.join(tradeoff_dir, f'individual_accuracy_{csv_suffix}_tradeoff.csv')
    aggregated_csv_path = os.path.join(tradeoff_dir, f'combined_accuracy_{csv_suffix}_tradeoff.csv')
    
    # Check if CSV files exist
    assert os.path.exists(individual_csv_path), \
        f"Individual tradeoff data not found: {individual_csv_path}. " \
        "Please run p090_tradeoff_compute.py first."
    
    assert os.path.exists(aggregated_csv_path), \
        f"Aggregated tradeoff data not found: {aggregated_csv_path}. " \
        "Please run p090_tradeoff_compute.py first."
    
    # Load CSV files
    import pandas as pd
    df_individual = pd.read_csv(individual_csv_path)
    df_aggregated = pd.read_csv(aggregated_csv_path)
    
    print(f"Loaded individual tradeoff data: {len(df_individual)} rows from {individual_csv_path}")
    print(f"Loaded aggregated tradeoff data: {len(df_aggregated)} rows from {aggregated_csv_path}")
    
    return df_individual, df_aggregated


def tradeoff_scatter_and_naive_baseline(base_chart: "alt.Chart", x_column: str, x_title: str, 
                                        accuracy_col: str, metric_name: str, naive_column: str,
                                        size_range: tuple[int, int] = (20, 200), scatter_opacity: float = 0.7, 
                                        size: int | None = None, baseline_stroke_width: int = 2, 
                                        baseline_opacity: float = 0.8) -> "tuple[alt.Chart, alt.LayerChart]":
    """
    Create both a scatter plot and naive baseline visualization with common styling.
    
    Args:
        base_chart: Base Altair chart
        x_column: Column name for x-axis data
        x_title: Title for x-axis
        accuracy_col: Column name for accuracy data
        metric_name: Name of the metric (e.g., 'HOTA', 'MOTA')
        naive_column: Column name for naive baseline data
        size_range: Tuple of (min, max) for tile size scale
        scatter_opacity: Opacity for the scatter points
        size: Fixed size for scatter points (if None, uses tile_size encoding)
        baseline_stroke_width: Width of the baseline rule line
        baseline_opacity: Opacity of the baseline rule line
        
    Returns:
        tuple[alt.Chart, alt.Chart]: Tuple of (scatter_plot, naive_baseline)
    """
    import altair as alt
    # Create scatter plot
    scatter = base_chart.mark_circle(opacity=scatter_opacity).encode(
        x=alt.X(f'{x_column}:Q', title=x_title),
        y=alt.Y(f'{accuracy_col}:Q', title=f'{metric_name} Score',
                scale=alt.Scale(domain=[0, 1])),
        color=alt.Color('classifier:N', title='Classifier'),
        tooltip=['video_name', 'classifier', 'tile_size', x_column, accuracy_col]
    ).properties(
        width=200,
        height=200
    )
    
    # Add size encoding only if no fixed size is provided
    if size is None:
        scatter = scatter.encode(size=alt.Size('tile_size:O',
                                 title='Tile Size',
                                 scale=alt.Scale(range=size_range)))
    
    # Create naive baseline
    baseline = base_chart.mark_rule(
        color='red',
        strokeDash=[5, 5],
        strokeWidth=baseline_stroke_width,
        opacity=baseline_opacity
    ).encode(
        x=f'{naive_column}:Q'
    )
    
    # Create annotation text for the baseline
    baseline_annotation = base_chart.mark_text(
        align='left',
        baseline='bottom',
        fontSize=12,
        fontWeight='bold',
        color='red',
        dx=5,  # Offset to the right of the line
        dy=0,  # No vertical offset
        angle=90  # Rotate 90 degrees clockwise
    ).encode(
        x=f'{naive_column}:Q',
        y=alt.value(0.5),  # Position at middle of y-axis
        text=alt.value('Without Optimization')
    )
    
    return scatter, baseline + baseline_annotation


METRICS = [
    'HOTA',
    # 'CLEAR',
    # 'Identity',
]


METRICS = ['HOTA', 'CLEAR']

DATASETS_TO_TEST = [
    # 'caldot1-yolov5',
    # 'caldot2-yolov5',
    # 'caldot1',
    # 'caldot2',
    'b3d-jnc00',
    # 'b3d-jnc02',
    # 'b3d-jnc06',
    # 'b3d-jnc07',
]


DATASETS_CHOICES = [
    'caldot1-yolov5',
    'caldot2-yolov5',
    'caldot1',
    'caldot2',
    'b3d-jnc00',
    'b3d-jnc02',
    'b3d-jnc06',
    'b3d-jnc07',
]


CLASSIFIERS_TO_TEST = [
    'SimpleCNN',
    'YoloN',
    # 'YoloS',
    # 'YoloM',
    # 'YoloL',
    # 'YoloX',
    'ShuffleNet05',
    # 'ShuffleNet20',
    # 'MobileNetL',
    'MobileNetS',
    # 'WideResNet50',
    # 'WideResNet101',
    # 'ResNet18', 
    # 'ResNet101',
    # 'ResNet152',
    # 'EfficientNetS',
    # 'EfficientNetL',
]

CLASSIFIERS_CHOICES = [
    # Cutsom CNNs
    'SimpleCNN',

    # YOLOv11 models
    'YoloN',
    'YoloS',
    'YoloM',
    'YoloL',
    'YoloX',

    # ShuffleNet models
    'ShuffleNet05',
    'ShuffleNet20',

    # MobileNet models
    'MobileNetL',
    'MobileNetS',

    # ResNet models
    'ResNet18',
    'ResNet101',
    'ResNet152',

    # WideResNet models
    'WideResNet50',
    'WideResNet101',

    # EfficientNet models
    'EfficientNetS',
    'EfficientNetL',
]


def get_classifier_from_name(classifier_name: str):
    """
    Get the classifier class based on the classifier name.
    
    Args:
        classifier_name (str): Name of the classifier to use
        
    Returns:
        The classifier class
        
    Raises:
        ValueError: If the classifier is not supported
    """
    if classifier_name == 'SimpleCNN':
        from polyis.models.classifier.simple_cnn import SimpleCNN
        return SimpleCNN
    elif classifier_name == 'YoloN':
        from polyis.models.classifier.yolo import YoloN
        return YoloN
    elif classifier_name == 'YoloS':
        from polyis.models.classifier.yolo import YoloS
        return YoloS
    elif classifier_name == 'YoloM':
        from polyis.models.classifier.yolo import YoloM
        return YoloM
    elif classifier_name == 'YoloL':
        from polyis.models.classifier.yolo import YoloL
        return YoloL
    elif classifier_name == 'YoloX':
        from polyis.models.classifier.yolo import YoloX
        return YoloX
    elif classifier_name == 'ShuffleNet05':
        from polyis.models.classifier.shufflenet import ShuffleNet05
        return ShuffleNet05
    elif classifier_name == 'ShuffleNet20':
        from polyis.models.classifier.shufflenet import ShuffleNet20
        return ShuffleNet20
    elif classifier_name == 'MobileNetL':
        from polyis.models.classifier.mobilenet import MobileNetL
        return MobileNetL
    elif classifier_name == 'MobileNetS':
        from polyis.models.classifier.mobilenet import MobileNetS
        return MobileNetS
    elif classifier_name == 'WideResNet50':
        from polyis.models.classifier.wide_resnet import WideResNet50
        return WideResNet50
    elif classifier_name == 'WideResNet101':
        from polyis.models.classifier.wide_resnet import WideResNet101
        return WideResNet101
    elif classifier_name == 'ResNet152':
        from polyis.models.classifier.resnet import ResNet152
        return ResNet152
    elif classifier_name == 'ResNet101':
        from polyis.models.classifier.resnet import ResNet101
        return ResNet101
    elif classifier_name == 'ResNet18':
        from polyis.models.classifier.resnet import ResNet18
        return ResNet18
    elif classifier_name == 'EfficientNetS':
        from polyis.models.classifier.efficientnet import EfficientNetS
        return EfficientNetS
    elif classifier_name == 'EfficientNetL':
        from polyis.models.classifier.efficientnet import EfficientNetL
        return EfficientNetL
    else:
        raise ValueError(f"Unsupported classifier: {classifier_name}")
