#!/usr/local/bin/python

import argparse
import json
import os
import shutil
import numpy as np
import cv2
import multiprocessing as mp
from functools import partial
from typing import Callable

from polyis.utilities import CACHE_DIR, CLASSIFIERS_CHOICES, ProgressBar, DATASETS_TO_TEST, TILE_SIZES


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - datasets (List[str]): Dataset names to process (default: ['b3d'])
    """
    parser = argparse.ArgumentParser(description='Unpack detection results from packed detections generated by 040_exec_detect.py')
    parser.add_argument('--datasets', required=False,
                        default=DATASETS_TO_TEST,
                        nargs='+',
                        help='Dataset names (space-separated)')
    return parser.parse_args()


def load_mapping_file(index_map_path: str, offset_lookup_path: str):
    """
    Load mapping file that contains the index_map and offset_lookup for unpacking.
    
    Args:
        index_map_path (str): Path to the index map file
        offset_lookup_path (str): Path to the offset lookup file
        
    Returns:
        tuple[np.ndarray, list[tuple[tuple[int, int], tuple[int, int], int]]]: Mapping information containing index_map, offset_lookup, etc.
        
    Raises:
        FileNotFoundError: If index map or offset lookup file doesn't exist
    """
    if not os.path.exists(index_map_path):
        raise FileNotFoundError(f"Index map file not found: {index_map_path}")
    if not os.path.exists(offset_lookup_path):
        raise FileNotFoundError(f"Offset lookup file not found: {offset_lookup_path}")
    
    index_map = np.load(index_map_path)
    with open(offset_lookup_path, 'r') as f:
        offset_lookup: list[tuple[tuple[int, int], tuple[int, int], int]] = [json.loads(line) for line in f]
    
    return index_map, offset_lookup


Det = list[float]
Dets = list[Det]
FrameIdToDets = dict[int, Dets]
UnpackedDets = tuple[FrameIdToDets, Dets, Dets]


def unpack_detections(detections: list[list[float]], index_map: np.ndarray, 
                      offset_lookup: list[tuple[tuple[int, int], tuple[int, int], int]], 
                      tilesize: int) -> UnpackedDets:
    """
    Unpack detections from compressed coordinates back to original frame coordinates.
    
    Args:
        detections (list[list[float]]): list of bounding boxes in compressed coordinates [x1, y1, x2, y2]
        index_map (np.ndarray): Index map from the mapping file (2D array with group_ids)
        offset_lookup (list[tuple[tuple[int, int], tuple[int, int], int]]): Offset lookup from the mapping file
        tilesize (int): Size of tiles used for compression
        
    Returns:
        tuple[
            dict[int, list[list[float]]],
            list[list[float]],
            list[list[float]],
        ]:
            dictionary mapping frame indices to lists of bounding boxes in original frame coordinates,
            list of detections that are not in any tile,
            list of detections that are in the center of a tile but not in any tile
    """
    # Initialize dictionary to store detections per frame
    frame_detections: dict[int, list[list[float]]] = {}
    not_in_any_tile_detections: list[list[float]] = []
    center_not_in_any_tile_detections: list[list[float]] = []

    # Process each detection
    for x1, y1, x2, y2 in detections:
        # Get the center point of the detection in compressed coordinates
        center_x = (x1 + x2) / 2.0
        center_y = (y1 + y2) / 2.0
        
        # center, top-left, top-right, bottom-left, bottom-right
        xs = [center_x, x1, x2, x1, x2]
        ys = [center_y, y1, y1, y2, y2]

        group_id: int | None = None
        frame_idx: int | None = None
        center_in_any_tile: bool = True
        for x, y in zip(xs, ys):
            # Convert to tile coordinates in the compressed image
            tile_x = int(x // tilesize)
            tile_y = int(y // tilesize)
            
            # Ensure tile coordinates are within bounds
            if (tile_y < 0 or tile_y >= index_map.shape[0] or 
                tile_x < 0 or tile_x >= index_map.shape[1]):
                continue
            
            # Get the group ID for this tile
            group_id_ = int(index_map[tile_y, tile_x])
            
            if group_id_ != 0:
                group_id = group_id_
                break
            center_in_any_tile = False
        
        if not center_in_any_tile:
            center_not_in_any_tile_detections.append([x1, y1, x2, y2])
        
        if group_id is None:
            not_in_any_tile_detections.append([x1, y1, x2, y2])
            continue

        # Convert group_id to 0-based index
        group_id -= 1

        # Get the offset information for this group
        assert 0 <= group_id < len(offset_lookup), f"Group {group_id} not found in offset lookup"
        (packed_y, packed_x), (original_offset_y, original_offset_x), frame_idx = offset_lookup[group_id]
        
        # Calculate the offset to convert from compressed to original coordinates
        # The offset represents how much the tile was moved during compression
        offset_x = (original_offset_x - packed_x) * tilesize
        offset_y = (original_offset_y - packed_y) * tilesize
        
        # Convert detection back to original frame coordinates
        original_det = [
            x1 + offset_x,
            y1 + offset_y,
            x2 + offset_x,
            y2 + offset_y,
        ]
        
        # Add to frame detections
        if frame_idx not in frame_detections:
            frame_detections[frame_idx] = []
        frame_detections[frame_idx].append(original_det)
    
    return frame_detections, not_in_any_tile_detections, center_not_in_any_tile_detections


def process_unpacking_task(video_file_path: str, tilesize: int, classifier: str,
                           tilepadding: bool, gpu_id: int, command_queue: mp.Queue):
    """
    Process unpacking for a single video/classifier/tilesize combination.
    This function is designed to be called in parallel.
    
    Args:
        video_file_path (str): Path to the video file directory
        tilesize (int): Tile size used for compression
        classifier (str): Classifier name used for compression and detection
        tilepadding (bool): Whether padding was applied to classification results
        gpu_id (int): GPU ID to use for processing
        command_queue (mp.Queue): Queue for progress updates
    """
    device = f'cuda:{gpu_id}'
    tilepadding_str = "padded" if tilepadding else "unpadded"
    
    # Check if compressed detections exist
    detections_file = os.path.join(video_file_path, '040_compressed_detections',
                                   f'{classifier}_{tilesize}_{tilepadding_str}', 'detections.jsonl')
    assert os.path.exists(detections_file)
    
    # Check if compressed frames directory exists
    compressed_frames_dir = os.path.join(video_file_path, '030_compressed_frames',
                                         f'{classifier}_{tilesize}_{tilepadding_str}')
    assert os.path.exists(compressed_frames_dir)
    
    # print(f"Processing video {video_file_path} for unpacking")
    
    detections_file = os.path.join(video_file_path, '040_compressed_detections',
                                   f'{classifier}_{tilesize}_{tilepadding_str}', 'detections.jsonl')
    
    unpacked_output_dir = os.path.join(video_file_path, '050_uncompressed_detections',
                                       f'{classifier}_{tilesize}_{tilepadding_str}')
    if os.path.exists(unpacked_output_dir):
        shutil.rmtree(unpacked_output_dir)
    os.makedirs(unpacked_output_dir, exist_ok=True)
    # print(f"Saving unpacked detections to {unpacked_output_dir}")
    
    images_not_in_any_tile_dir = os.path.join(unpacked_output_dir, 'images_not_in_any_tile')
    os.makedirs(images_not_in_any_tile_dir, exist_ok=True)
    # print(f"Saving images not in any tile to {images_not_in_any_tile_dir}")

    images_center_not_in_any_tile_dir = os.path.join(unpacked_output_dir, 'images_center_not_in_any_tile')
    os.makedirs(images_center_not_in_any_tile_dir, exist_ok=True)
    # print(f"Saving images center not in any tile to {images_center_not_in_any_tile_dir}")

    # dictionary to store all frame detections
    all_frame_detections: dict[int, list[list[float]]] = {}
    
    with open(detections_file, 'r') as f:
        # Process each detection file
        contents = f.readlines()
        description = f"{video_file_path} {tilesize:>3} {classifier:>{max(len(c) for c in CLASSIFIERS_CHOICES)}} {tilepadding_str}"
        kwargs = {'completed': 0, 'total': len(contents), 'description': description}
        mod = max(1, int(len(contents) * 0.05))
        command_queue.put((device, kwargs))
        for idx, line in enumerate(contents):
            content = json.loads(line)
            image_file: str = content['image_file']

            from_idx, to_idx = image_file.split('.')[0].split('_')
            from_idx = int(from_idx)
            to_idx = int(to_idx)

            # Construct paths
            index_map_path = os.path.join(compressed_frames_dir, 'index_maps', f'{from_idx:08d}_{to_idx:08d}.npy')
            offset_lookup_path = os.path.join(compressed_frames_dir, 'offset_lookups', f'{from_idx:08d}_{to_idx:08d}.jsonl')
            
            # Load detection results
            detections: list[list[float]] = content['bboxes']
            
            # Load corresponding mapping file
            index_map, offset_lookup = load_mapping_file(index_map_path, offset_lookup_path)
            
            # Unpack detections
            (
                frame_detections,
                not_in_any_tile_detections,
                center_not_in_any_tile_detections,
            ) = unpack_detections(detections, index_map, offset_lookup, tilesize)

            # save not_in_any_tile_detections and center_not_in_any_tile_detections
            if len(not_in_any_tile_detections) > 0:
                # load the image
                image_path = os.path.join(compressed_frames_dir, 'images', image_file)
                image = cv2.imread(image_path)
                assert image is not None, f"Image not found: {image_path}"

                # draw the detections
                for det in not_in_any_tile_detections:
                    x1, y1, x2, y2 = det
                    image = cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)

                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    image = cv2.circle(image, (int(center_x), int(center_y)), 5, (0, 0, 255), -1)
                
                # save the image
                cv2.imwrite(os.path.join(images_not_in_any_tile_dir, image_file), image)

            if len(center_not_in_any_tile_detections) > 0:
                # load the image
                image_path = os.path.join(compressed_frames_dir, 'images', image_file)
                image = cv2.imread(image_path)
                assert image is not None, f"Image not found: {image_path}"

                # draw the detections
                for det in center_not_in_any_tile_detections:
                    x1, y1, x2, y2 = det
                    image = cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)

                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    image = cv2.circle(image, (int(center_x), int(center_y)), 5, (0, 0, 255), -1)
                
                # save the image
                cv2.imwrite(os.path.join(images_center_not_in_any_tile_dir, image_file), image)
            
            # Merge with existing frame detections
            for frame_idx, bboxes in frame_detections.items():
                if frame_idx not in all_frame_detections:
                    all_frame_detections[frame_idx] = []
                all_frame_detections[frame_idx].extend(bboxes)

            if idx % mod == 0:
                command_queue.put((device, {'completed': idx + 1,
                                            'description': description,
                                            'total': len(contents)}))
    
    # Save unpacked detections organized by frame
    # Sort frames by index
    sorted_frames = sorted(all_frame_detections.keys())
    
    # Save each frame's detections
    with open(os.path.join(unpacked_output_dir, 'detections.jsonl'), 'w') as f:
        for frame_idx in sorted_frames:
            bboxes = all_frame_detections[frame_idx]
            f.write(json.dumps({ 'frame_idx': frame_idx, 'bboxes': bboxes }) + '\n')

    # print(f"Saved unpacked detections for {len(sorted_frames)} frames")


def main(args):
    """
    Main function that orchestrates the detection unpacking process using parallel processing.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directories exist
    2. Creates a list of all video/classifier/tilesize combinations to process
    3. Uses multiprocessing to process tasks in parallel across available GPUs
    4. Processes each video and saves unpacked detection results
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - datasets (List[str]): Names of the datasets to process
            
    Note:
        - The script expects compressed detections from 040_exec_detect.py in:
          {CACHE_DIR}/{dataset}/execution/{video_file}/040_compressed_detections/{classifier}_{tilesize}/detections.jsonl
        - The script expects mapping files from 030_exec_compress.py in:
          {CACHE_DIR}/{dataset}/execution/{video_file}/030_compressed_frames/{classifier}_{tilesize}/
        - Unpacked detections are saved to:
          {CACHE_DIR}/{dataset}/execution/{video_file}/050_uncompressed_detections/{classifier}_{tilesize}/detections.jsonl
        - Each line in the output JSONL file contains one bounding box [x1, y1, x2, y2] in original frame coordinates
        - All available video/classifier/tilesize combinations are processed
        - If no compressed detections are found for a video/tilesize/classifier combination, that combination is skipped
        - The number of processes equals the number of available GPUs
    """
    # mp.set_start_method('spawn', force=True)
    
    # Create tasks list with all video/classifier/tilesize combinations
    funcs: list[Callable[[int, mp.Queue], None]] = []

    for dataset_name in args.datasets:
        dataset_dir = os.path.join(CACHE_DIR, dataset_name, 'execution')
        
        if not os.path.exists(dataset_dir):
            print(f"Dataset directory {dataset_dir} does not exist, skipping...")
            continue
        
        # Get all video files from the dataset directory
        video_files = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]
        
        for video_file in sorted(video_files):
            video_file_path = os.path.join(dataset_dir, video_file)
            
            compressed_detections_dir = os.path.join(video_file_path, '040_compressed_detections')
            if not os.path.exists(compressed_detections_dir):
                print(f"No compressed detections directory found for {video_file}, skipping")
                continue
        
            uncompressed_detections_dir = os.path.join(video_file_path, '050_uncompressed_detections')
            if os.path.exists(uncompressed_detections_dir):
                shutil.rmtree(uncompressed_detections_dir)
                
            for classifier_tilesize_tilepadding in sorted(os.listdir(compressed_detections_dir)):
                classifier, tilesize, tilepadding_str = classifier_tilesize_tilepadding.split('_')
                tilesize = int(tilesize)
                tilepadding = tilepadding_str == "padded"
                funcs.append(partial(process_unpacking_task, video_file_path, tilesize, classifier, tilepadding))
    
    print(f"Created {len(funcs)} tasks to process")
    
    # Set up multiprocessing with ProgressBar
    # Use number of available CPUs as the number of processes
    num_processes = int(mp.cpu_count() * 0.5)
    if len(funcs) < num_processes:
        num_processes = len(funcs)
    
    num_processes = 20
    
    ProgressBar(num_workers=num_processes, num_tasks=len(funcs), refresh_per_second=5).run_all(funcs)
    print("All tasks completed!")


if __name__ == '__main__':
    main(parse_args())
