#!/usr/local/bin/python

import argparse
import json
import os
import shutil
import time
import cv2
import multiprocessing as mp
from functools import partial
from typing import Callable
import torch

import polyis.models.detector
from polyis.utilities import CACHE_DIR, format_time, CLASSIFIERS_CHOICES, CLASSIFIERS_TO_TEST, ProgressBar


# TILE_SIZES = [30, 60, 120]
TILE_SIZES = [30, 60]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - datasets (List[str]): Dataset names to process (default: ['b3d'])
            - tile_size (str): Tile size to use for detection (choices: '30', '60', '120', 'all')
            - detector: Detector is auto-selected based on dataset name
            - classifiers (list): Classifier names to use (default: CLASSIFIERS_TO_TEST + ['groundtruth'])
            - clear (bool): Whether to remove and recreate the 040_compressed_detections folder for each video (default: False)
    """
    parser = argparse.ArgumentParser(description='Detect objects from packed images generated by 030_exec_pack.py')
    parser.add_argument('--datasets', required=False,
                        default=['caldot1', 'caldot2'],
                        nargs='+',
                        help='Dataset names (space-separated)')
    parser.add_argument('--tile_size', type=str, choices=['30', '60', '120', 'all'], default='all',
                        help='Tile size to use for detection (or "all" for all tile sizes)')
    # Detector selection is now automatic based on dataset name
    parser.add_argument('--classifiers', required=False,
                        default=CLASSIFIERS_TO_TEST + ['groundtruth'],
                        # default=['groundtruth'],
                        choices=CLASSIFIERS_CHOICES + ['groundtruth'],
                        nargs='+',
                        help='Classifier names to use (can specify multiple): '
                             f'{", ".join(CLASSIFIERS_CHOICES + ["groundtruth"])}. For example: '
                             '--classifiers YoloN ShuffleNet05 ResNet18 groundtruth')
    parser.add_argument('--clear', action='store_true',
                        help='Remove and recreate the 040_compressed_detections folder for each video')
    return parser.parse_args()


def detect_objects(video_file_path: str, tile_size: int, classifier: str,
                   dataset_name: str, gpu_id: int, command_queue: mp.Queue):
    """
    Detect objects in compressed images using auto-selected detector.
    
    Args:
        video_file_path (str): Path to the video file
        tile_size (int): Tile size used for compression
        classifier (str): Classifier name used for compression
        dataset_name (str): Name of the dataset (used to auto-select detector)
        gpu_id (int): GPU ID to use for processing
        command_queue (mp.Queue): Queue for progress updates
    """
    device = f'cuda:{gpu_id}'
    video_name = os.path.basename(video_file_path)
    
    compressed_frames_dir = os.path.join(video_file_path, '030_compressed_frames', f'{classifier}_{tile_size}', 'images')
    assert os.path.exists(compressed_frames_dir)

    detector = polyis.models.detector.get_detector(dataset_name, gpu_id)

    print(f"Processing video {video_file_path}")

    # Create output directory for detections
    detections_output_dir = os.path.join(video_file_path, '040_compressed_detections', f'{classifier}_{tile_size}')
    if os.path.exists(detections_output_dir):
        # Remove the entire directory
        shutil.rmtree(detections_output_dir)
    os.makedirs(detections_output_dir, exist_ok=True)

    # Get all compressed image files
    image_files = [f for f in os.listdir(compressed_frames_dir) if f.endswith('.jpg')]
    
    if not image_files:
        raise FileNotFoundError(f"No compressed images found in {compressed_frames_dir}")

    with (open(os.path.join(detections_output_dir, 'detections.jsonl'), 'w') as f,
          open(os.path.join(detections_output_dir, 'runtimes.jsonl'), 'w') as fr):
        kwargs = {'completed': 0,
                  'total': len(image_files),
                  'description': f"{video_name} {tile_size:>3} {classifier}"}
        command_queue.put((device, kwargs))
        for idx, image_file in enumerate(image_files):
            image_path = os.path.join(compressed_frames_dir, image_file)
            runtime = dict()
            
            # Read the compressed image
            start_time = (time.time_ns() / 1e6)
            frame = cv2.imread(image_path)
            if frame is None:
                raise ValueError(f"Could not read image {image_path}")
            end_time = (time.time_ns() / 1e6)
            runtime['read'] = end_time - start_time

            # Detect objects in the frame
            start_time = (time.time_ns() / 1e6)
            outputs = polyis.models.detector.detect(frame, detector)
            end_time = (time.time_ns() / 1e6)
            runtime['detect'] = end_time - start_time

            # Extract bounding boxes (x1, y1, x2, y2 format)
            bounding_boxes = outputs[:, :4].tolist()

            # Save detection results
            f.write(json.dumps({ 'image_file': image_file, 'bboxes': bounding_boxes }) + '\n')
            fr.write(json.dumps(format_time(**runtime)) + '\n')

            command_queue.put((device, {'completed': idx + 1}))


def main(args):
    """
    Main function that orchestrates the object detection process on compressed images using parallel processing.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directories exist
    2. Creates a list of all video/classifier/tile_size combinations to process
    3. Uses multiprocessing to process tasks in parallel across available GPUs
    4. Processes each video and saves detection results
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - datasets (List[str]): Names of the datasets to process
            - tile_size (str): Tile size to use for detection ('30', '60', '120', or 'all')
            - detector: Detector is auto-selected based on dataset name
            - classifiers (list): List of classifier names to use (default: CLASSIFIERS_TO_TEST + ['groundtruth'])
            - clear (bool): Whether to remove and recreate the 040_compressed_detections folder for each video
            
    Note:
        - The script expects compressed images from 030_exec_compress.py in:
          {CACHE_DIR}/{dataset}/execution/{video_file}/030_compressed_frames/{classifier}_{tile_size}/images/
        - Detection results are saved to:
          {CACHE_DIR}/{dataset}/execution/{video_file}/040_compressed_detections/{classifier}_{tile_size}/detections.jsonl
        - Each line in the output JSONL file contains one bounding box [x1, y1, x2, y2]
        - When tile_size is 'all', all available tile sizes are processed
        - When classifiers is not specified, all classifiers in CLASSIFIERS_TO_TEST + ['groundtruth'] are processed
        - If no compressed images are found for a video/tile_size/classifier combination, that combination is skipped
        - The number of processes equals the number of available GPUs
    """
    mp.set_start_method('spawn', force=True)
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        tile_sizes_to_process = TILE_SIZES
        print(f"Processing all tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    # Determine which classifiers to process
    classifiers_to_process = args.classifiers
    
    # Create tasks list with all video/classifier/tile_size combinations
    funcs: list[Callable[[int, mp.Queue], None]] = []
    
    for dataset_name in args.datasets:
        dataset_dir = os.path.join(CACHE_DIR, dataset_name, 'execution')
        
        if not os.path.exists(dataset_dir):
            print(f"Dataset directory {dataset_dir} does not exist, skipping...")
            continue
        
        # Show detector info for this dataset
        detector_info = polyis.models.detector.get_detector_info(dataset_name)
        print(f"Using detector: {detector_info['detector']} ({detector_info['description']})")
        
        # Get all video files from the dataset directory
        video_files = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]
        
        for video_file in sorted(video_files):
            video_file_path = os.path.join(dataset_dir, video_file)
            
            # Clear compressed detections folder if requested
            if args.clear:
                compressed_detections_base_dir = os.path.join(video_file_path, '040_compressed_detections')
                if os.path.exists(compressed_detections_base_dir):
                    shutil.rmtree(compressed_detections_base_dir)
                    print(f"Cleared existing 040_compressed_detections folder: {compressed_detections_base_dir}")
            
            for classifier in classifiers_to_process:
                for tile_size in tile_sizes_to_process:
                    # Check if compressed frames directory exists
                    compressed_frames_dir = os.path.join(video_file_path, '030_compressed_frames', f'{classifier}_{tile_size}', 'images')
                    if not os.path.exists(compressed_frames_dir):
                        print(f"No compressed frames directory found for {video_file} {classifier} {tile_size}, skipping")
                        continue
                    
                    funcs.append(partial(detect_objects, video_file_path, tile_size, classifier, dataset_name))
    
    print(f"Created {len(funcs)} tasks to process")
    
    # Set up multiprocessing with ProgressBar
    # Use number of available GPUs as the number of processes
    num_gpus = torch.cuda.device_count()
    assert num_gpus > 0, "No GPUs available"
    print(f"Using {num_gpus} GPUs for parallel processing")
    
    if len(funcs) < num_gpus:
        num_gpus = len(funcs)
    
    ProgressBar(num_workers=num_gpus, num_tasks=len(funcs)).run_all(funcs)
    print("All tasks completed!")


if __name__ == '__main__':
    main(parse_args())
