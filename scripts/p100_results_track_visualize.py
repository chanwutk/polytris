#!/usr/local/bin/python

import argparse
import json
import os
import cv2
import numpy as np
from tqdm import tqdm
import multiprocessing as mp

from polyis.utilities import CACHE_DIR, DATA_DIR, create_tracking_visualization


TILE_SIZES = [64]


def parse_args():
    parser = argparse.ArgumentParser(description='Visualize tracking results from 060_exec_track.py on original videos')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['64', '128', 'all'], default='all',
                        help='Tile size to use for visualization (or "all" for all tile sizes)')
    parser.add_argument('--speed_up', type=int, default=4,
                        help='Speed up factor for visualization (process every Nth frame)')
    return parser.parse_args()


def load_tracking_results(cache_dir: str, dataset: str, video_file: str, tile_size: int) -> dict[int, list[list[float]]]:
    """
    Load tracking results from the JSONL file generated by 060_exec_track.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for tracking
        
    Returns:
        dict[int, list[list[float]]]: dictionary mapping frame indices to lists of tracks
        
    Raises:
        FileNotFoundError: If no tracking results file is found
    """
    tracking_path = os.path.join(cache_dir, dataset, video_file, 'uncompressed_tracking',
                                f'proxy_{tile_size}', 'tracking.jsonl')
    
    if not os.path.exists(tracking_path):
        raise FileNotFoundError(f"Tracking results not found: {tracking_path}")
    
    print(f"Loading tracking results from: {tracking_path}")
    
    frame_tracks = {}
    with open(tracking_path, 'r') as f:
        for line in f:
            if line.strip():
                frame_data = json.loads(line)
                frame_idx = frame_data['frame_idx']
                tracks = frame_data['tracks']
                frame_tracks[frame_idx] = tracks
    
    print(f"Loaded tracking results for {len(frame_tracks)} frames")
    return frame_tracks



def process_video_visualization(video_file: str, tile_size: int, cache_dir: str, dataset: str, speed_up: int, process_id: int):
    """
    Process visualization for a single video file.
    
    Args:
        video_file (str): Name of the video file to process
        tile_size (int): Tile size used for tracking
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        speed_up (int): Speed up factor for visualization
        process_id (int): Process ID for logging
    """
    # Load tracking results
    tracking_results = load_tracking_results(cache_dir, dataset, video_file, tile_size)
    
    # Get path to original video
    video_path = os.path.join(DATA_DIR, dataset, video_file)
    
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Original video not found for {video_file}")
    
    # Create output path for visualization
    output_path = os.path.join(cache_dir, dataset, video_file, 'uncompressed_tracking',
                                f'proxy_{tile_size}', 'visualization.mp4')
    
    # Create visualization
    create_tracking_visualization(video_path, tracking_results, output_path, speed_up, process_id)


def main(args):
    """
    Main function that orchestrates the tracking visualization process.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directory exists
    2. Finds all videos with tracking results from 060_exec_track.py for the specified tile size(s)
    3. Creates a process pool for parallel processing
    4. Creates visualizations for each video and saves results
    
    Args:
        args (argparse.Namespace): Parsed command line arguments
        
    Note:
        - The script expects tracking results from 060_exec_track.py in:
          {CACHE_DIR}/{dataset}/{video_file}/uncompressed_tracking/proxy_{tile_size}/tracking.jsonl
        - Original videos are read from {DATA_DIR}/{dataset}/
        - Visualization videos are saved to:
          {CACHE_DIR}/{dataset}/{video_file}/uncompressed_tracking/proxy_{tile_size}/visualization.mp4
        - Each track ID gets a unique color from a predefined palette
        - Processing is parallelized for improved performance
        - When tile_size is 'all', all available tile sizes are processed
    """
    print(f"Processing dataset: {args.dataset}")
    print(f"Tile size: {args.tile_size}")
    print(f"Speed up factor: {args.speed_up} (processing every {args.speed_up}th frame)")
    
    # Determine which tile sizes to process
    tile_sizes_to_process = TILE_SIZES if args.tile_size == 'all' else [int(args.tile_size)]
    
    # Find all videos with tracking results
    dataset_cache_dir = os.path.join(CACHE_DIR, args.dataset)
    
    # Look for directories that contain tracking results
    video_tile_combinations = []
    for item in os.listdir(dataset_cache_dir):
        item_path = os.path.join(dataset_cache_dir, item)
        if os.path.isdir(item_path):
            for tile_size in tile_sizes_to_process:
                tracking_path = os.path.join(item_path, 'uncompressed_tracking', f'proxy_{tile_size}', 'tracking.jsonl')
                if os.path.exists(tracking_path):
                    video_tile_combinations.append((item, tile_size))
    
    if not video_tile_combinations:
        print(f"No videos with tracking results found in {dataset_cache_dir}")
        return
    
    print(f"Found {len(video_tile_combinations)} video-tile size combinations to process")
    
    # Determine number of processes to use
    num_processes = min(mp.cpu_count(), len(video_tile_combinations), 20)  # Cap at 20 processes
    
    # Prepare arguments for each video-tile combination
    video_args = []
    for i, (video_file, tile_size) in enumerate(video_tile_combinations):
        process_id = i % num_processes  # Assign process ID in round-robin fashion
        video_args.append((video_file, tile_size, CACHE_DIR, args.dataset, args.speed_up, process_id))
        print(f"Prepared video: {video_file} with tile size: {tile_size} for process {process_id}")
    
    # Use process pool to execute video visualization
    with mp.Pool(processes=num_processes) as pool:
        print(f"Starting video visualization with {num_processes} parallel workers...")
        
        # Map the work to the pool
        results = pool.starmap(process_video_visualization, video_args)
        
        print("All videos visualized successfully!")


if __name__ == '__main__':
    main(parse_args())
