#!/usr/bin/env python3
"""
Script to compare compression results across different configurations.

This script analyzes compressed frames generated by compression scripts and
creates comparison tables:
1. Number of compressed images for each (dataset, classifier, tilesize, tilepadding, stage)
2. Number of empty vs occupied tiles for each configuration
3. Runtime per operation for each configuration
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from polyis.utilities import CACHE_DIR, DATASETS_TO_TEST, get_video_frame_count


INPUT_DIRS = ['030_compressed_frames', '031_compressed_frames', '032_compressed_frames', '033_compressed_frames']
OUTPUT_DIR = '083_compress'


def parse_args():
    parser = argparse.ArgumentParser(
        description='Compare compression results across different configurations'
    )
    parser.add_argument(
        '--datasets',
        nargs='+',
        default=DATASETS_TO_TEST,
        help='Datasets to analyze (default: all datasets in DATASETS_TO_TEST)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Print verbose output'
    )
    return parser.parse_args()


def build_config_paths_dataframe(datasets: List[str], verbose: bool = False) -> pd.DataFrame:
    """
    Build a DataFrame containing all configuration paths across datasets and input directories.

    Args:
        datasets: List of dataset names
        verbose: Whether to print verbose output

    Returns:
        DataFrame with columns: dataset, video, config_path, config_name, stage
    """
    # Collect all configuration paths
    config_records = []

    # Scan all datasets, videos, input directories, and configurations
    for dataset in datasets:
        dataset_cache_dir = Path(CACHE_DIR) / dataset / 'execution'

        # Find all video directories
        for video_dir in dataset_cache_dir.iterdir():
            video_name = video_dir.name
            if not video_name.startswith('te'):
                continue

            # Scan all input directories
            for input_dir in INPUT_DIRS:
                compressed_frames_dir = video_dir / input_dir
                # Find all configuration directories
                for config_dir in compressed_frames_dir.iterdir():
                    config_records.append({
                        'dataset': dataset,
                        'video': video_name,
                        'config_path': str(config_dir),
                        'config_name': config_dir.name,
                        'stage': input_dir
                    })

    # Create DataFrame from collected records
    df = pd.DataFrame(config_records)

    if verbose:
        print(f"\nFound {len(df)} configurations across {df['dataset'].nunique()} datasets")
        print(f"Datasets: {df['dataset'].unique().tolist()}")
        print(f"Stages: {df['stage'].unique().tolist()}")

    return df


def parse_config_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Parse configuration names to extract classifier, tilesize, and tilepadding.

    Args:
        df: DataFrame with config_name column

    Returns:
        DataFrame with added columns: classifier, tilesize, tilepadding
    """
    # Split config names using pandas string operations
    config_parts = df['config_name'].str.split('_', expand=True)
    config_parts.columns = ['classifier', 'tilesize', 'tilepadding']

    # Convert tilesize to int
    config_parts['tilesize'] = config_parts['tilesize'].astype(int)

    # Concatenate with original DataFrame
    result_df = pd.concat([df, config_parts], axis=1)

    return result_df


def count_images_for_path(config_path: str) -> int:
    """
    Count the number of compressed images in a configuration directory.

    Args:
        config_path: Path to configuration directory

    Returns:
        Number of .jpg files in the images subdirectory
    """
    # Use pathlib for cleaner path handling
    images_dir = Path(config_path) / 'images'

    # Count .jpg files using glob
    jpg_files = list(images_dir.glob('*.jpg'))
    return len(jpg_files)


def count_tiles_for_path(config_path: str) -> Tuple[int, int]:
    """
    Count empty and occupied tiles from index_maps in a configuration directory.

    Args:
        config_path: Path to configuration directory

    Returns:
        Tuple of (empty_tiles, occupied_tiles)
    """
    # Use pathlib for cleaner path handling
    index_maps_dir = Path(config_path) / 'index_maps'

    # Get all .npy files
    npy_files = list(index_maps_dir.glob('*.npy'))

    # Load all index maps and compute counts using vectorized numpy operations
    empty_tiles = 0
    occupied_tiles = 0

    for npy_file in npy_files:
        # Load the index map
        index_map = np.load(str(npy_file))

        # Use numpy vectorized operations for counting
        empty_tiles += int(np.sum(index_map == 0))
        occupied_tiles += int(np.sum(index_map != 0))

    return int(empty_tiles), int(occupied_tiles)


def get_video_total_frames(row: pd.Series) -> int:
    """
    Get the total number of frames for a video.

    Args:
        row: DataFrame row with 'dataset' and 'video' columns

    Returns:
        Total number of frames in the video
    """
    # Use the get_video_frame_count function from utilities
    dataset = str(row['dataset'])
    video = str(row['video'])
    return get_video_frame_count(dataset, video)


def parse_runtime_for_path(config_path: str) -> Dict[str, float]:
    """
    Parse runtime.jsonl file and aggregate per-operation times.

    Args:
        config_path: Path to configuration directory

    Returns:
        Dictionary mapping operation names to total time in seconds
    """
    # Use pathlib for cleaner path handling
    runtime_file = Path(config_path) / 'runtime.jsonl'

    # Aggregate runtime per operation
    op_times: Dict[str, float] = {}

    with open(runtime_file, 'r') as f:
        for line in f:
            if not line.strip():
                continue
            entry = json.loads(line)
            # Extract runtime data - format is {'frame_idx': ..., 'runtime': [{'op': ..., 'time': ...}, ...]}
            runtime_data = entry.get('runtime', [])
            assert isinstance(runtime_data, list), f"Runtime data is not a list for {runtime_file}"
            for op_data in runtime_data:
                assert isinstance(op_data, dict), f"Op data is not a dict for {runtime_file}"
                assert 'op' in op_data and 'time' in op_data, f"Op data is missing op or time for {runtime_file}"
                op_name = op_data['op']
                # Time is stored in milliseconds, convert to seconds
                op_time_ms = float(op_data['time'])
                op_time_s = op_time_ms / 1000.0
                op_times[op_name] = op_times.get(op_name, 0.0) + op_time_s

    return op_times


def compute_metrics(df: pd.DataFrame, verbose: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Compute image counts, tile counts, and runtime metrics for all configurations.

    Args:
        df: DataFrame with config_path column
        verbose: Whether to show progress bar

    Returns:
        Tuple of:
        - DataFrame with added columns: num_images, empty_tiles, occupied_tiles,
                                       total_tiles, occupancy_ratio, total_frames,
                                       and runtime columns per operation
        - DataFrame with long-format runtime data: dataset, video, stage, classifier,
                                                   tilesize, tilepadding, op, runtime
    """
    if verbose:
        print("\nComputing video frame counts...")

    # Get total frames for each video (only need to compute once per unique dataset+video combination)
    # Check for duplicates and raise error if found
    video_subset = df[['dataset', 'video']]
    unique_videos = video_subset.drop_duplicates().copy()
    tqdm.pandas(desc="Getting video frame counts", disable=not verbose)
    unique_videos['total_frames'] = unique_videos.progress_apply(get_video_total_frames, axis=1) if verbose else unique_videos.apply(get_video_total_frames, axis=1)

    # Merge the total_frames back into the main DataFrame
    merge_df = unique_videos[['dataset', 'video', 'total_frames']].copy()  # type: ignore
    df = df.merge(merge_df, on=['dataset', 'video'], how='left')  # type: ignore

    if verbose:
        print("Computing image counts...")

    # Apply count_images function to each row
    tqdm.pandas(desc="Counting images", disable=not verbose)
    df['num_images'] = df['config_path'].progress_apply(count_images_for_path) if verbose else df['config_path'].apply(count_images_for_path)

    if verbose:
        print("Computing tile counts...")

    # Apply count_tiles function and expand the tuple result into two columns using pandas
    tqdm.pandas(desc="Counting tiles", disable=not verbose)
    tile_counts = df['config_path'].progress_apply(count_tiles_for_path) if verbose else df['config_path'].apply(count_tiles_for_path)
    # Convert Series of tuples to DataFrame using pandas from_records
    tile_df = pd.DataFrame.from_records(tile_counts.values, columns=['empty_tiles', 'occupied_tiles'], index=df.index)
    df = pd.concat([df, tile_df], axis=1)

    # Compute derived metrics using vectorized pandas operations
    df['total_tiles'] = df['empty_tiles'] + df['occupied_tiles']
    df['occupancy_ratio'] = df['occupied_tiles'] / df['total_tiles'].where(df['total_tiles'] > 0, 1)

    if verbose:
        print("Computing runtime metrics...")

    # Parse runtime files and extract per-operation times
    tqdm.pandas(desc="Parsing runtime files", disable=not verbose)
    runtime_dicts = df['config_path'].progress_apply(parse_runtime_for_path) if verbose else df['config_path'].apply(parse_runtime_for_path)

    # Convert Series of dictionaries to DataFrame using json_normalize
    # This creates columns for each operation automatically
    assert len(runtime_dicts) > 0 and any(runtime_dicts), "No runtime data found"
    runtime_df = pd.json_normalize(runtime_dicts.tolist())
    runtime_df.columns = [f'runtime_{col}' for col in runtime_df.columns]
    runtime_df.index = df.index
    df = pd.concat([df, runtime_df], axis=1)
    df['runtime_total'] = df[df.columns[df.columns.str.startswith('runtime_')]].sum(axis=1)

    # Create long-format runtime dataframe with one row per operation
    # Select columns for identifier and runtime columns (excluding runtime_total)
    id_cols = ['dataset', 'video', 'stage', 'classifier', 'tilesize', 'tilepadding']
    runtime_cols = [col for col in df.columns if col.startswith('runtime_') and col != 'runtime_total']
    
    # Melt the runtime columns into long format
    runtime_df = df[id_cols + runtime_cols].copy()
    runtime_df = runtime_df.melt(
        id_vars=id_cols,
        value_vars=runtime_cols,
        var_name='op',
        value_name='runtime'
    )
    # Remove 'runtime_' prefix from operation names
    runtime_df['op'] = runtime_df['op'].str.replace('runtime_', '', regex=False)

    # drop rows with NaN runtime values
    runtime_df = runtime_df.dropna(subset=['runtime'])
    return df, runtime_df


def save_comparison_tables(df: pd.DataFrame, runtime_df: pd.DataFrame, verbose: bool = False):
    """
    Save comparison tables as CSV files for each dataset.

    Args:
        df: DataFrame with all metrics
        runtime_df: DataFrame with long-format runtime data (dataset, video, stage, classifier, tilesize, tilepadding, op, runtime)
        verbose: Whether to print verbose output
    """
    # Group by dataset and save separate CSVs
    for dataset, group_df in df.groupby('dataset'):
        # Create output directory
        dataset_str = str(dataset)
        output_dir = Path(CACHE_DIR) / dataset_str / 'evaluation' / OUTPUT_DIR
        output_dir.mkdir(parents=True, exist_ok=True)

        # Select columns for image counts table (include stage)
        image_cols = ['dataset', 'video', 'stage', 'classifier', 'tilesize', 'tilepadding', 'num_images', 'total_frames']
        image_df = group_df[image_cols].copy()

        # Select columns for tile counts table (include stage)
        tile_cols = ['dataset', 'video', 'stage', 'classifier', 'tilesize', 'tilepadding',
                     'empty_tiles', 'occupied_tiles', 'total_tiles', 'occupancy_ratio']
        tile_df = group_df[tile_cols].copy()

        # Save image counts table
        image_csv_path = output_dir / 'image_counts_comparison.csv'
        image_df.to_csv(image_csv_path, index=False)
        print(f"\nSaved image counts table: {image_csv_path}")

        if verbose:
            print(f"  Total configurations: {len(image_df)}")
            print(f"  Total images: {image_df['num_images'].sum()}")
            print(f"  Total frames: {image_df['total_frames'].sum()}")

        # Save tile counts table
        tile_csv_path = output_dir / 'tile_counts_comparison.csv'
        tile_df.to_csv(tile_csv_path, index=False)
        print(f"Saved tile counts table: {tile_csv_path}")

        if verbose:
            print(f"  Total configurations: {len(tile_df)}")
            print(f"  Total tiles: {tile_df['total_tiles'].sum()}")
            print(f"  Average occupancy ratio: {tile_df['occupancy_ratio'].mean():.4f}")

        # Save runtime table
        runtime_csv_path = output_dir / 'runtime_comparison.csv'
        runtime_df.to_csv(runtime_csv_path, index=False)
        print(f"Saved runtime table: {runtime_csv_path}")

        if verbose:
            print(f"  Total configurations: {len(group_df)}")
            print(f"  Total runtime: {group_df['runtime_total'].sum():.2f} seconds")
            print(f"  Average total runtime: {group_df['runtime_total'].mean():.2f} seconds")


def main(args):
    """
    Main function to compare compression results.

    Args:
        args: Parsed command line arguments
    """
    if args.verbose:
        print("Starting compression comparison analysis...")
        print(f"Cache directory: {CACHE_DIR}")
        print(f"Datasets to analyze: {args.datasets}")

    # Step 1: Build DataFrame of all configuration paths
    df = build_config_paths_dataframe(args.datasets, args.verbose)

    if df.empty:
        print("\nNo compression data found for any dataset!")
        return

    # Step 2: Parse configuration names to extract classifier, tilesize, tilepadding
    df = parse_config_names(df)

    # Step 3: Compute metrics (image counts and tile counts) using pandas apply
    df, runtime_df = compute_metrics(df, args.verbose)

    # Step 4: Save comparison tables grouped by dataset
    save_comparison_tables(df, runtime_df, args.verbose)

    # Step 5: Print summary using pandas aggregation functions
    print(f"\nSummary:")
    print(f"  Total datasets processed: {df['dataset'].nunique()}")
    print(f"  Total configurations analyzed: {len(df)}")
    print(f"  Total stages: {df['stage'].nunique()}")
    print(f"  Total compressed images: {df['num_images'].sum()}")
    print(f"  Total tiles: {df['total_tiles'].sum()}")
    if 'runtime_total' in df.columns:
        print(f"  Total runtime: {df['runtime_total'].sum():.2f} seconds")


if __name__ == '__main__':
    args = parse_args()
    main(args)